---

layout:     post
title:      java多线程进阶
subtitle:   java多线程进阶
date:       2019-10-11
author:     skaleto
catalog: true
tags:
    - java，多线程

---

# 

[TOC]



## 并发编程的挑战

#### 上下文切换

在单核处理器中，如果要实现多线程，其实在CPU内部还是串行处理的，CPU通过时间片分配算法来切换每一个任务。在切换前会保存上一个任务的状态，下次切换回这个任务时会加载这个状态。因此线程的一次切换会对应一次上下文的切换，这种切换是会影响执行效率的。



##### 减少上下文切换的方法

- 无锁并发编程，例如将数据的ID按照Hash取模分段，不同县城处理不同的数据
- CAS算法，CAS算法不需要加锁
- 使用最少线程
- 协程，在单线程中使用多个协程来实现多任务调度



#### 避免死锁的几个方法

- 避免一个线程同时获取多个锁
- 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源
- 使用定时锁，使用tryLock(timeout)
- 数据库锁的加锁和解锁必须在一个数据库连接里



#### 资源限制的挑战

##### 资源限制

在进行并发编程时，程序的执行速度受限于计算机硬件或软件资源

##### 如何解决资源限制的问题

对于硬件资源限制，考虑使用集群来并行执行程序，通过“数据ID%机器数”来得到一个机器编号；

对于软件资源限制，考虑使用资源池将资源进行复用，例如数据库连接池，socket连接复用

- 



## Java并发机制的底层原理

#### Volatile使用优化

在LinkedTransferQueue中（jdk1.7 concurrent包中新增），内部类PaddedAtomicReference（但是我从源码中并没有找到）中将共享变量追加到了64字节。原因是在大多数CPU平台上，高速缓存行是64个字节宽。这就意味着，当一个队列的共享变量大小不足64字节时，例如4个字节时，在多个处理器的情况下，每个处理器都需要4个字节的空间，那么就会存在一个缓存行中存放了多个处理器中的共享变量。在对变量进行修改时，处理器会将一个缓存行锁定，这就意味着其他处理器没有办法访问这个缓存行中自己存放的内容了。假如使用64字节，就可以保证每个处理器都占用一个缓存行。

*不得不说Doug lea太强了*



#### Java对象头

synchronized用的锁是存放在Java对象头中的。

数组类型对象，使用3字宽存储对象头；非数组类型对象，使用2字宽存储对象头，具体结构如下图。

*在32位虚拟机中，1字宽=4字节=32bit

![1571642327984](..\img\multithread\1571642327984.png)

##### Mark Word

存放了对象的hashcode、分代年龄和锁标记位

- 32-bit下，Mark Word为32bit长，可能存在下面四种不同的存储结构

  ![1571642571768](..\img\multithread\1571642571768.png)

- 64-bit下，Mark Word为64bit长，存在下面两种情况

  ![1571642716775](..\img\multithread\1571642716775.png)



#### Java的几种锁

##### 偏向锁

例如ReentrantLock

大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获取，因此为了让同一线程获取锁的代价更低，就引入了偏向锁。

当线程获取锁时，会在对象头和栈帧中的锁记录里存储偏向的线程ID，当下次这个线程尝试进入和退出锁定的块时，不需要CAS操作来加锁解锁，只需要校验一下记录的线程ID是否和当前一致即可。

###### 撤销偏向锁

偏向锁实际上加锁后就一直没有释放，直到有其他的线程来竞争这个锁，当有其他线程来竞争时，会先暂停偏向锁的线程（前提是当前没有正在执行的字节码，否则暂停肯定会有问题），偏向锁要么重新偏向其他线程，要么被设置为无锁状态或者标记对象不适合用偏向锁。

![1571647325177](..\img\multithread\1571647325177.png)

###### 关闭偏向锁

偏向锁在Java6,7中默认启动，但是是延迟启动，可以用-XX:BiasedLockingStartupDelay=0来关闭延迟；可以用-XX:-UseBiasedLocking=false来关闭偏向锁，并进入轻量级锁



##### 自旋锁

假如某个锁的持有时间比较短，那么假设A线程在竞争锁的时候，该锁被B线程占用着，正常情况下A线程需要阻塞等待锁释放，但由于前提条件锁的持有时间比较短，A线程此时不阻塞，而是进行自旋（例如执行空的for循环），在自旋的过程中不断尝试去获取锁，假如自旋过程中正常获得锁了，那么线程A就正常执行；假如自旋过程中没有获得锁，那么线程A就只能阻塞。

这种自旋方式等待锁是有上面所说的前提的，假如锁的持有时间比较长，或者是计算密集型的线程，那么使用自旋锁是得不偿失的，因为不断自旋会消耗CPU。并且，在单核处理器上，由于不存在真正的多线程，因此多个线程之间必然要阻塞，也是不能使用自旋锁的。

JVM也对自旋锁做了优化，假如线程A在上一次竞争锁的时候成功了，那么下一次竞争的时候自旋的时间相应会变长，反之自旋时间会变短。这种方式叫做自适应自旋锁，是建立在JVM认为每个线程持有锁的时间基本相当的假设上的。



##### 轻量级锁

线程在执行同步块前，JVM会先在当前线程的栈帧中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，然后线程尝试用CAS将对象头中的MarkWord替换为指向锁记录的指针，成功则当前线程获得锁，失败则其他线程在竞争锁，当前线程自旋来获取锁。

轻量级锁是建立在当前系统中实际上没有锁的竞争存在的假设上，这种锁仅需要在对象头中记录状态，不需要实际向系统申请互斥量。

![1571749798845](..\img\multithread\1571749798845.png)



##### 重量级锁

常用的synchronized，在jdk5之前就是重量级锁，直接向系统申请互斥量，竞争失败的线程直接阻塞；

而在jdk6之后，synchronized内部也加入了偏向锁和轻量级锁的优化。



#### 原子操作

##### CPU术语定义

缓存行（cache line）：缓存的最小操作单位

比较和交换（Compare and Swap，CAS操作）：输入一个旧值一个新值，先比较旧值有没有变化，如果没有变化则改为新值，如果有变化则不改变

内存顺序冲突（Memory order violation）：多个CPU同时修改一个缓存行，引起其中的一个CPU操作无效

##### 处理器实现原子操作

- 总线锁，当一个处理器在使用总线输出信号时，其他处理器的请求会被阻塞，使得每个处理器可以独占共享内存
- 缓存锁，如果内存区域在缓存行上，那么它在锁定期间处理器不在总线上声明lock，只是修改内存地址并保证缓存的一致性

##### JAVA实现原子操作

- 循环CAS操作，例如自旋CAS的实现就是循环进行CAS直到成功为止，例如java并发包中的AtomicBoolean，AtomicInteger等等
  - ABA问题，如果一个值从A变为B又变为A，那么CAS会认为没有发生变化，解决办法就是为每个值加上版本号，例如AtomicStampedReference
  - 循环CAS操作十分消耗CPU
  - CAS只能对一个变量进行原子操作，解决办法就是把多个变量合成一个对象来进行原子操作，例如jdk的AtomicReference可以放入多个变量来进行原子操作
- 使用锁机制来实现，java中除了偏向锁，其他锁在加锁的时候也是使用循环CAS来获取锁，解锁的时候使用循环CAS来释放锁



## JAVA内存模型

java线程的内存模型在介绍volatile关键字的时候已经介绍过。

![1571813781568](..\img\multithread\1571813781568.png)



#### 指令重排序

程序执行过程中，为了提高性能，编译器和处理器会对指令进行重排序

- 编译器优化的重排序，编译器在不改变单线程语义的前提下重新安排执行顺序

- 指令级并行的重排序，现代处理器采用指令级并行技术使得多条指令可以重叠执行

- 内存系统的重排序，处理器根据缓存和读写缓冲区进行重排序（由于存在缓冲区，实际操作可能读发生在写之前，导致读到的内容并不是写缓存中即将更新的内容）

JMM（java内存模型）要求Java编译器在生成指令序列时，插入特定类型的内存屏障，来禁止特定类型的处理器重排序，内存屏障有如下几种类型：

![1571814699000](..\img\multithread\1571814699000.png)

在java模型中，用happens-before来阐述这些不同重排序下内存的可见性



#### 数据依赖性

如果两个操作同时访问一个变量，并且两个操作中有一个是写操作，那么这两个操作之间是有数据依赖性的，编译器为了保证在单线程场景下，执行效果一致，不会对存在数据依赖性的两个操作进行指令重排序，因为那样很有可能会改变最终结果

```java
//下面三个操作，前两个指令可以互换位置而不会影响最终结果，但是最后一个指令不可以被放到前面
double pi = 3.14;
double r = 1.0;
double area = pi * r * r;

//下面这个操作中，在单线程环境下1,2之间、3，4之间没有数据依赖性，所以有可能发生指令重排；
//当多线程环境下，很容易发现，有可能falg已经被设置为true但a还未更新，这种情形下得到的i是0
class ReorderExample {
	int a = 0;
	boolean flag = false;
	public void writer() {
		a = 1;         // 1
		flag = true;   // 2
	}
    
	Public void reader() {
		if (flag) {    // 3
		int i = a * a; // 4
		}
	}
}
```



#### volatile的内存语义

为了实现volatile的内存语义，JMM会分别限制编译器重排序和处理器重排序，规则如下

![1571822161761](..\img\multithread\1571822161761.png)

从表中我们可以看到几个特点：

- 当第二个操作是volatile写时，不管前面操作是什么，都不能重排序，保证了volatile写一定是最后的操作
- 当第一个操作是volatile读时，不管后面操作是什么，都不能重排序，保证了volatile读一定是最先发生的
- 当第一个操作是volatile写，第二个操作时volatile读时，不能重排序

JMM在指令序列前加入内存屏障来禁止指令重排

- 在每个volatile写操作的前面插入一个StoreStore屏障
- 在每个volatile写操作的后面插入一个StoreLoad屏障
- 在每个volatile读操作的后面插入一个LoadLoad屏障
- 在每个volatile读操作的后面插入一个LoadStore屏障

那么volatile写在插入内存屏障后就变成下面的指令序列

![1571822512969](..\img\multithread\1571822512969.png)





#### 锁的内存语义

以ReentrantLock为例，它的实现依赖于Java同步器框架AbstractQueuedSynchronizer（本文简称之为
AQS）。AQS使用一个整型的volatile变量（命名为state）来维护同步状态，这个变量是实现ReentrantLock内存语义的关键。

```java
//-------------------------公平锁--------------------------
private volatile int state;
/**
 * 加锁
 * ReentrantLock#lock() -> FairSync:lock() -> AbstractQueuedSynchronizer#acquire(int arg) -> ReentrantLock#tryAcquire(int acquires)
 */
protected final boolean tryAcquire(int acquires) {
	final Thread current = Thread.currentThread();
	int c = getState();　　　　// 获取锁的开始，首先读volatile变量state
	if (c == 0) {
		if (isFirst(current) && compareAndSetState(0, acquires)) {
			setExclusiveOwnerThread(current);
			return true;
		}
	} else if (current == getExclusiveOwnerThread()) {
		int nextc = c + acquires;
		if (nextc < 0)　
			throw new Error("Maximum lock count exceeded");
			setState(nextc);
			return true;
		}
	return false;
}


/**
 * 解锁
 * ReentrantLock#unlock() -> AbstractQueuedSynchronizer#release(int arg) -> Sync#tryRelease(int releases)
 */
protected final boolean tryRelease(int releases) {
	int c = getState() - releases;
	if (Thread.currentThread() != getExclusiveOwnerThread())
		throw new IllegalMonitorStateException();
		boolean free = false;
		if (c == 0) {
			free = true;
			setExclusiveOwnerThread(null);
		}
		setState(c);　　　　　// 释放锁的最后，写volatile变量state
		return free;
}


//-------------------------非公平锁---------------------------
/**
 * 加锁
 * ReentrantLock:lock() -> NonfairSync:lock() -> AbstractQueuedSynchronizer:compareAndSetState(int expect,int update)
 */
protected final boolean compareAndSetState(int expect, int update) {
    //这里调用了系统实现的CAS操作，本质上同时具有volatile读和volatile写操作
	return unsafe.compareAndSwapInt(this, stateOffset, expect, update);
}
```

不管是公平锁还是非公平锁，我们都可以看到在加锁时会存在一个volatile读或volatile读写，意味着在它们之后，会存在内存屏障，禁止指令的重排，内存屏障会持续到解锁时的volatile写。

Java线程之间的通信现在有了下面4种方式：

- A线程写volatile变量，随后B线程读这个volatile变量
- A线程写volatile变量，随后B线程用CAS更新这个volatile变量
- A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量（因为CAS同时有volatile读和写的内存语义）
- A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量

Java concurrent包中的多数基础类都会按照下面的模式来实现

- 首先，声明共享变量为volatile
- 然后，使用CAS的原子条件更新来实现线程之间的同步
- 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信

![1571904423456](..\img\multithread\1571904423456.png)



#### final的内存语义

由于java中final具有不可更改的特性，意味着在final的值初始化（初始化要么在变量定义的时候，要么在类的构造方法中）的过程中，不能发生重排，否则可能出现得到的final值不一致的问题，因此会在final读和写的前后加入内存屏障。但是这个语义在JSR-133上才被加入，保证了在不使用同步(lock或volatile)的情况下，就能保证任意线程能看到正常初始化的final值。



#### happens-before

JMM最核心的概念。

开发者希望内存模型易于理解（强内存模型）；而编译器和处理器希望内存模型对他们的束缚越小越好，以便于他们进行优化（弱内存模型）。JMM就是希望在这里面找到一个平衡。

按照这样的思路，JMM提出了一种happens-before规则，程序员通过这种规则可以理解并进行内存可见性开发。但在内部，JMM对两种情况作了分别处理，对于会改变程序执行结果的重排序，JMM会禁止处理器和编译器进行重排，对于其他不影响执行结果的重排序，JMM不会去做限制。

![1571906803476](..\img\multithread\1571906803476.png)

![1571907488556](..\img\multithread\1571907488556.png)



#### 再看单例模式下的双重检查锁

```java
public class DoubleCheckedLocking {                     // 1
	private static Instance instance;                   // 2
	public static Instance getInstance() {              // 3
		if (instance == null) {                         // 4:第一次检查
			synchronized (DoubleCheckedLocking.class) { // 5:加锁
				if (instance == null)                   // 6:第二次检查
					instance = new Instance();          // 7:问题的根源出在这里
			}                                           // 8
		}                                               // 9
		return instance;                                // 10
	}                                                   // 11
}
```

上面是一个双重检查锁单例的错误示范。

一个对象的创建分为下面三个步骤：

- 分配对象的内存空间
- 初始化对象
- 设置instance指向刚分配的内存地址

上面的三个步骤由于在单线程场景下重排不会影响结果，因此编译器是有可能对它进行重排的，也就意味着有可能instance已经指向了某个内存地址，但还未完成初始化，但在其他线程里已经可以看到这个对象不为null，所以就拿去用了，但是其实对象还没有初始化完成，直接使用就会出现问题。

所以我们有两种思路避免这个问题：

1. 不允许步骤2和3进行指令重排
2. 允许指令重排，但不允许其他线程看到这个重排



对于思路1，我们使用一个volatile来修饰instance，就可以在最后加上一个volatile写屏障，禁止中间的指令重排

对于思路2，我们可以使用静态方法或静态内部类的方式进行实例的初始化，由于JAVA内部对类的初始化由锁来控制同时只能有一个线程进行初始化，变相的也解决了这个问题，但是在这种方式中我们看到，并没有禁止指令的重排，只是对于其他的线程不可见了。



## 锁

### 队列同步器

队列同步器AbstractQueuedSynchronizer是用来构建锁或其他同步组件的基础框架，事实上是一个模板方法，通过实现其中的几个重要方法，可以自定义锁，参考下方

```java
public class MyMutexLock implements Lock {

    private static class Sync extends AbstractQueuedSynchronizer {
        private static final int UNLOCKED = 0;
        private static final int LOCKED = 1;

        //检查当前在排他模式下，锁是否被占用
        @Override
        protected boolean isHeldExclusively() {
            return getState() == LOCKED;
        }

        @Override
        protected boolean tryAcquire(int arg) {
            //CAS检查当前状态是否是0，0表示锁没有被占用，于是将它设置为1占用状态
            if (compareAndSetState(UNLOCKED, LOCKED)) {
                setExclusiveOwnerThread(Thread.currentThread());
                return true;
            }
            return false;
        }

        @Override
        protected boolean tryRelease(int arg) {
            if (getState() == UNLOCKED) {
                throw new IllegalMonitorStateException();
            }
            setState(UNLOCKED);
            return true;
        }
        //new Condition可以分配给一个线程，用来实现等待和通知机制，唤醒某个特定的线程
        private Condition newCondition() {
            return new ConditionObject();
        }
    }

    private final Sync sync = new Sync();

    @Override
    public void lock() {
        //同步器的acquire方法为final，实际上调用了我们重写的tryAcquire
        sync.acquire(1);
    }

    @Override
    public void lockInterruptibly() throws InterruptedException {
        //同步器的acquireInterruptibly方法为final，实际上调用了我们重写的tryAcquire
        sync.acquireInterruptibly(1);
    }

    @Override
    public boolean tryLock() {
        return sync.tryAcquire(1);
    }

    @Override
    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException {
        //同步器的tryAcquireNanos方法为final，实际上调用了我们重写的tryAcquire
        return sync.tryAcquireNanos(1, unit.toNanos(time));
    }

    @Override
    public void unlock() {
        sync.tryRelease(1);
    }

    @Override
    public Condition newCondition() {
        return sync.newCondition();
    }
}
```

##### 独占式同步状态的获取

在同步器中，可以看到一个Node内部类，事实上，同步器就是在自己内部维护了一个FIFO的双向队列，队列拥有头和尾节点，假如一个线程竞争锁失败，它将构造成节点并被加入到队列的尾部

```java
//头节点
private transient volatile Node head;
//尾节点
private transient volatile Node tail;
/**
 *      +------+  prev +-----+       +-----+
 * head |      | <---- |     | <---- |     |  tail
 *      +------+       +-----+       +-----+
 */
static final class Node {
       
        static final Node SHARED = new Node();
        static final Node EXCLUSIVE = null;
    	//某个等待的节点线程等待超时或被中断，则将该节点设置为取消等待
        static final int CANCELLED =  1;
    	//当前节点如果释放了同步状态或取消，则会通知后继节点使后继节点得以运行
        static final int SIGNAL    = -1;
    	//当前节点在等待队列中，节点线程等待在Condition上，当Condition被signal后将从等待队列转移到同步队列上
        static final int CONDITION = -2;
    	//下一次共享式同步状态获取将无条件被传播下去
        static final int PROPAGATE = -3;
    	//waitStatus从上面的几种常量中指定
        volatile int waitStatus;

        volatile Node prev;
        volatile Node next;
    
    	//被构造成节点的线程对象，将在Node的构造方法中带入
        volatile Thread thread;
		//等待队列中的后继节点，如果当前节点是共享的，这个字段将是SHARED常量
        Node nextWaiter;

    	//...
}
```

由于每个Node在被加入队列中时也需要保证线程安全，否则会出现问题，因此在同步器中有一个方法compareAndSetTail用来同步地将节点加入到队列中

```java
private final boolean compareAndSetTail(Node expect, Node update) {
    return unsafe.compareAndSwapObject(this, tailOffset, expect, update);
}
```

那么锁是怎么获取的呢？来看acquire方法

```java
public final void acquire(int arg) {
    //第一步，tryAcquire获得独占锁
    if (!tryAcquire(arg) &&
        //第二步，假如独占锁获取失败，则构造成Node节点并加入同步队列
        //第三步，在同步队列中自旋
        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
        //第四步，将当前线程置为中断，见下方疑难详解
        selfInterrupt();
}

//将当前线程按照给定的模式构造成节点并接入到队列尾部
private Node addWaiter(Node mode) {
    Node node = new Node(Thread.currentThread(), mode);
    Node pred = tail;
    if (pred != null) {
        node.prev = pred;
        //先尝试进行一次CAS设置尾部节点
        if (compareAndSetTail(pred, node)) {
            pred.next = node;
            return node;
        }
    }
    //如果上面的设置节点操作失败，那么说明此时有多线程在进行节点添加操作，此时调用enq进行死循环操作直到当前节点设置成功
    enq(node);
    return node;
}

final boolean acquireQueued(final Node node, int arg) {
    boolean failed = true;
    try {
        boolean interrupted = false;
        //不断自旋判断并尝试获取锁
        for (;;) {
            //获得前驱节点，如果为空则抛出空指针
            final Node p = node.predecessor();
            //当前驱节点为头节点则尝试获取锁
            if (p == head && tryAcquire(arg)) {
                //获取成功的话就把当前节点设置为头节点并且将前节点释放，因为需要满足FIFO
                setHead(node);
                p.next = null; // help GC
                failed = false;
                return interrupted;
            }
            if (shouldParkAfterFailedAcquire(p, node) &&
                //检查是否需要将线程挂起，如果需要则线程挂起，则调用LockSupport的park方法，比较类似于线程的wait方法，将线程设置为挂起状态
                parkAndCheckInterrupt())
                interrupted = true;
        }
    } finally {
       if (failed)
            cancelAcquire(node);
    }
}

private final boolean parkAndCheckInterrupt() {
    LockSupport.park(this);
    return Thread.interrupted();
}
```

疑难详解：

在第三步和第四步的代码中有一个比较绕的地方，即为什么在第三步中需要检查线程的中断状态，在第四步中需要再将自身的线程置为中断？原因是这样的，在第三步中，使用了LockSupport.park()方法将线程挂起，但事实上，假如当前线程处于中断状态，park()方法将没有效果直接返回，也就是说如果不把这个中断状态抹掉，将在死循环中不停地进行park操作而永远没有结果，这将非常消耗CPU性能。

因此，在parkAndCheckInterrupt()方法中执行park后，调用了Thread.interrupted()方法返回并抹掉中断状态，这使得在下一次循环中，park操作将执行成功。

最后，在第四步里，当acquireQueued()方法的返回值为true，即对应着之前Thread.interrupted()获得到的线程中断状态为true，假如有中断过，因为在park操作的时候把它抹掉了，那么在aqcuire操作结束之前，需要把线程的中断状态给他恢复回去，否则就影响了线程原本的状态。



整个过程流程图如下

![1573097724169](..\img\multithread\1573097724169.png)

![1573098049767](..\img\multithread\1573098049767.png)

##### 独占式同步状态的释放

```java
public final boolean release(int arg) {
    if (tryRelease(arg)) {
        Node h = head;
        if (h != null && h.waitStatus != 0)
            //调用LockSupport.unpark唤醒当前节点后面正在等待的节点线程
            unparkSuccessor(h);
        return true;
    }
    return false;
}
```

在使用synchronized实现锁时，存在一个问题，假如当前线程已经阻塞在等待锁的过程中，此时线程被中断，那么这个线程等待锁的过程不会结束，会一直进行下去，而在同步器中，acquireInterruptibly会在自旋获取锁的过程中检查线程是否已经被中断，假如被中断，则会抛出异常。



##### 共享式同步状态的获取和释放

共享锁与独占式锁的实现方式比较类似，也是采用队列和自旋，仅仅是tryAcquire的方式不太一样；另外共享锁的释放也和独占锁的释放不太一样，独占锁因为在获得锁之后，当前仅有一个线程持有，因此释放不需要考虑线程安全的问题，而共享锁因为会存在多个线程共同持有，因此需要考虑线程安全的问题。





#### ReentrantLock

我们都知道ReentrantLock是一个可重入锁，换句话说当一个线程持有了一个ReentrantLock后，假如再次尝试获取锁，它也能成功获取，可重入的性质在synchronized方式下隐式实现了（因为synchronized关注的是对象监视器，一个线程获得某个对象监视器后，只要不释放，他将一直获得）。但在ReentrantLock中，是像如下一样手动实现的。

```java
final boolean nonfairTryAcquire(int acquires) {
    final Thread current = Thread.currentThread();
    int c = getState();
    if (c == 0) {
        if (compareAndSetState(0, acquires)) {
            setExclusiveOwnerThread(current);
            return true;
        }
    //检查当前线程是否是上次得到锁的线程，如果是则将同步状态增加并返回成功
    } else if (current == getExclusiveOwnerThread()) {
        int nextc = c + acquires;
        if (nextc < 0) // overflow
            throw new Error("Maximum lock count exceeded");
        setState(nextc);
        return true;
    }
    return false;
}
```

在上面的代码上看到每一次重入都会对应着同步状态的增加，相对应的在释放的时候也需要进行相应数量同步状态的相减，否则锁并不能成功释放，这也应证在前面说到的使用这种锁的时候一般lock与unlock需要成对使用



##### ReentrantLock公平锁与非公平锁

公平锁即真正意义上的FIFO，锁的获得和释放将严格按照队列的顺序一个一个执行；而非公平锁的获得和释放不完全按照FIFO的顺序，即当前获得的线程在释放锁之后，下一次再获得锁的几率会更大，当然假如当前锁在释放之后永远不去请求锁了，那么后面的线程仍然会按照队列里面的顺序逐个获取，下面是公平锁和非公平锁的测试结果，可以看到非公平锁同一个线程可能多次重复获得锁。

![1573119054798](..\img\multithread\1573119054798.png)

虽然理解上来看，非公平锁可能导致后面的线程等待时间变长，但是它减少了线程切换的次数，减少了CPU的消耗，提高了吞吐量。



#### ReentrantReadWriteLock读写锁

读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。

读写锁同样也需要依赖同步器来实现，那么问题在于，同步器中只有一个state能够用来表示当前的锁状态，如何让这一个int属性值代表两种数据呢，设计者将state的高16位设计成读状态，低16位设计成写状态，于是，当读状态需要加1时，需要执行state+1<<16的操作，而写状态需要加1时，需要执行state+1的操作，并且通过不同的掩码来获取当前的状态值。

**这里在我看来，应该结合业务场景的考虑，如果读操作较多，可以考虑将低16位设计成读状态，以减少移位操作（不过操作系统执行移位操作本身也并不怎么消耗性能，所以也就无所谓了）*

##### 写锁的获取

```java
final boolean tryWriteLock() {
            Thread current = Thread.currentThread();
            int c = getState();
            if (c != 0) {
                int w = exclusiveCount(c);
                //获取方式和ReentrantLock很相似，差别在这里，需要额外判断一下当前是否有读锁存在，如果有其他的读锁，那么本次写锁将无法获取
                if (w == 0 || current != getExclusiveOwnerThread())
                    return false;
                if (w == MAX_COUNT)
                    throw new Error("Maximum lock count exceeded");
            }
            if (!compareAndSetState(c, c + 1))
                return false;
            setExclusiveOwnerThread(current);
            return true;
}
```

##### 读锁的获取

```java
final boolean tryReadLock() {
            Thread current = Thread.currentThread();
            for (;;) {
                int c = getState();
                //判断写锁是否存在，如果存在，将不可获得读锁
                if (exclusiveCount(c) != 0 &&
                    getExclusiveOwnerThread() != current)
                    return false;
                int r = sharedCount(c);
                if (r == MAX_COUNT)
                    throw new Error("Maximum lock count exceeded");
                if (compareAndSetState(c, c + SHARED_UNIT)) {
                    if (r == 0) {
                        firstReader = current;
                        firstReaderHoldCount = 1;
                    } else if (firstReader == current) {
                        firstReaderHoldCount++;
                    } else {
                        HoldCounter rh = cachedHoldCounter;
                        if (rh == null || rh.tid != getThreadId(current))
                            cachedHoldCounter = rh = readHolds.get();
                        else if (rh.count == 0)
                            readHolds.set(rh);
                        rh.count++;
                    }
                    return true;
                }
            }
}
```

##### 锁降级

锁降级的过程如下：

1. 获得写锁
2. 获得读锁
3. 释放写锁

```java
public void processData() {
	readLock.lock();
	if (!update) {
		// 必须先释放读锁
		readLock.unlock();
		// 锁降级从写锁获取到开始
		writeLock.lock();
		try {
			if (!update) {
				// 准备数据的流程
				update = true;
			}
			readLock.lock();
		} finally {
			writeLock.unlock();
		}
		// 锁降级完成，写锁降级为读锁
	}
	try {
		// 使用数据的流程
		} finally {
			readLock.unlock();
		}
}
```

这里面首先获得了写锁，进行数据的操作，当操作完成之后，并不立即释放写锁，而是在当前线程中获得读锁，注意这个地方并不与锁的特性冲突，在没有释放写锁的情况下，其他线程确实是无法获得读锁也无法获得写锁，但是不影响当前线程获得读锁，第三步再将写锁释放掉，那么当前线程事实上只有一个读锁，其他线程也可以获得读锁并看到最新的数据。

第二与第三步不可以顺序颠倒，假如释放写锁在前，那么其他线程可能很快再次拿到写锁，导致读锁依然拿不到，数据始终无法访问。

ReentrantReadWriteLock支持锁降级，但不支持锁升级，理由很简单，假如多个线程获得了读锁，此时其中的某个线程获得了写锁并对数据进行了更新，则此次更新对其他获得读锁的线程不可见，会带来很严重的后果。



#### LockSupport

LockSupport提供了最基本的线程阻塞和唤醒操作，是构建同步组件的基础工具

- park()，阻塞当前线程，只有当unpark被调用或线程中断才会返回
- parkNanos(long nanos)，有超时时间的阻塞
- parkUntil(long deadline)，有截止时间的阻塞
- park(Object blocker)，1.6加入
- parkNanos(Object blocker,long nanos)，1.6加入
- parkUntil(Object blocker,long deadline)，1.6加入
- unpark(Thread thread)，唤醒线程

blocker在1.6中加入是为了使线程dump、后能有更多阻塞对象的信息提供给开发者



#### Condition

在上面的介绍中，我们知道了在使用Lock的时候，可以使用Condition来实现等待和通知操作，与synchronized实现的等待通知机制不同的是，每个Lock对象都可以创建各自的Condition对象，从而实现唤醒特定的线程，这一点相比较于synchronized实现的对象监视器锁不同，不是由系统随机选择一个正在等待的线程进行唤醒。

在Condition中，复用了同步器中的Node构造了一个等待队列，当调用Condition.await()时，当前线程将被构造为节点，加入Condition的等待队列尾部。Condition中节点的更新不需要CAS操作，原因是await时，线程已经拿到了锁，是线程安全的。

在Object的监视器模型上，一个对象拥有一个同步队列和一个等待队列，而一个Lock拥有一个同步队列和多个等待队列，同时，同步器上的每个Condition都可以访问同步器中的方法。

![1573183802986](..\img\multithread\1573183802986.png)



##### 等待

当触发等待Condition.await()时，当前线程必须先获得锁，因此当前线程一定存在于同步队列的头部，此时将他构造成节点加入等待队列中，并将当前线程从同步队列头部去除，唤醒同步队列后面的节点。同时释放锁。

![1573184034560](..\img\multithread\1573184034560.png)

##### 通知

当触发通知Condition.signal()时，当前线程必须先获得锁（注意这里的当前线程指的是通知者，而不是被通知者），随后将等待队列中的头部节点中的线程构造成新的节点并加入到同步队列中，并唤醒这个线程，进行新的锁的竞争。

![1573184515113](..\img\multithread\1573184515113.png)

signalAll()方法则是从头开始对等待队列中的每个节点执行一次signal，并依次加入到同步队列中。





## Java并发容器和框架

### ConcurrentHashMap

*见HashMap源码解析*

### ConcurrentLinkedQueue

ConcurrentLinkedQueue是一个基于链接节点的无界线程安全队列，它采用先进先出的规则对节点进行排序，当我们添加一个元素的时候，它会添加到队列的尾部；当我们获取一个元素时，它会返回队列头部的元素。它采用了循环CAS操作来实现。

ConcurrentLinkedQueue中使用了一个FIFO的队列来维护队列中的元素，同样的也存在一个head和tail节点，用来快速找到队列的头部和尾部，方便元素的添加和弹出，那么要实现一个线程安全的队列，主要的问题在于如何实现线程安全的入队和出队操作。

##### 入队

![1573194755060](..\img\multithread\1573194755060.png)

入队操作的主要过程如上图，可以归纳为两点：

1. 定位队列中的尾节点
2. 设置入队节点为尾节点

从图中可以看出在入队操作时，tail节点并不是像我们常规理解的那样随着节点的添加而修改，而是在添加元素时，假如tail的下一个节点不为空时，tail将在元素被添加完成后被设置为这个元素（如上图添加元素2）；假如tail的下一个节点为空时，tail在元素被添加完成后也不会被设置成新元素（如上图添加元素3）。

从图上来看，4次添加元素操作，仅两次tail元素的修改，因为tail元素的读写都需要CAS操作来保证线程安全，这种设计使得增加了寻找真正结尾元素的操作，但减少了修改tail元素的操作，即增加了volatile读，减少了volatile写，提高了入队的效率。

##### 出队

![1573200447486](..\img\multithread\1573200447486.png)

出队操作也可以归纳为两点：

1. 定位队列的头元素
2. 弹出头元素

同样的，head节点的更新也和tail节点的更新方式类似，每隔两个节点才进行CAS节点修改



### 阻塞队列

阻塞队列支持阻塞的插入方法（当队列满时，队列会阻塞插入元素的线程，直到队列不满）和阻塞的移除方法（在队列为空时，获取元素的线程会等待队列变为非空），常用于生产者消费者场景

#### ArrayBlockingQueue

数组实现的有界阻塞队列，FIFO，默认情况下不保证线程公平的访问队列，即当队列可用时，阻塞的线程都可以争夺访问队列的资格，底层同步方式使用ReentrantLock实现

#### LinkedBlockingQueue

链表实现的有界阻塞队列，FIFO

#### PriorityBlockingQueue

支持优先级的无界阻塞队列，默认采用自然顺序升序排列，也可以自定义元素排序规则

#### DelayQueue

支持延时获取元素的无界阻塞队列，队列中的元素需要实现Delayed接口，在创建元素时可以指定多久才能从队列中获取当前元素，只有在延迟期满时才能从队列中提取元素

DelayQueue可以运用在下面的场景中：

- 缓存系统的设计，为元素分配有效期，使用线程循环获取元素，当能获取元素时说明该元素缓存有效期到了
- 定时任务的调度，配置队列中任务的开始时间，一旦从中取到任务时就可以开始任务，例如TimerQueue

#### SynchronousQueue

不存储元素的阻塞队列，每一个put操作必须等待一个take操作，否则不能继续添加元素，比较适合传递性的场景。默认使用非公平策略访问队列，也可以设置为公平策略

#### LinkedTransferQueue

链表实现的无界阻塞队列，多了tryTransfer和transfer方法

- transfer，如果当前有消费者阻塞着在等待接收元素，transfer方法可以将生产者传入的元素直接传给消费者；如果当前没有消费者在等待，则放入队列尾部，等待该元素被消费后返回
- tryTransfer，如果当前有消费者在等待接收，则将元素传给消费者；如果没有消费者在等待，则立即返回false

#### LinkedBlockingDeque

链表实现的双向阻塞队列



#### 阻塞队列的实现原理

使用锁的等待、通知机制即可







### Fork/Join框架

Java7提供的一个用于并行执行任务的框架，可以将大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架

#### 工作窃取算法

工作窃取算法是指某个线程从其他队列里窃取任务来执行，在执行大任务时，某个线程可能先执行完成，但是最终结果需要等待所有线程都执行完毕，那么此线程与其等待结束，不如帮其他线程处理任务，基于这种想法出现了工作窃取算法。

要实现一个工作窃取算法，那么需要一个用于存放所有任务的数据结构，支持所有线程从中取任务，一般会使用一个双端队列用来维护任务数据，被窃取的线程从队列尾部取任务处理，而窃取任务的线程从队列头部取，这样可以减少线程之间的竞争。

工作窃取算法的优点：充分利用线程进行并行计算，减少了线程间的竞争。
工作窃取算法的缺点：在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且该算法会消耗了更多的系统资源，比如创建多个线程和多个双端队列。

#### Fork/Join框架的设计

实现Fork/Join分为两个步骤

1. 分割任务
2. 合并每个子任务的执行结果

在Fork/Join中，可以创建RecursiveTask（有返回结果的任务）或RecursiveAction（没有返回结果的任务），并且在Task中分割或处理任务；同时使用ForkJoinPool创建用于执行任务的线程池。

https://blog.csdn.net/tyrroo/article/details/81390202

https://www.imooc.com/article/24822









