---

layout:     post
title:      从paxos到zookeeper
subtitle:   分布式
date:       2020-01-09
author:     skaleto
catalog: true
tags:
    - 分布式

---



## Paxos

paxos算法的基本流程如下图，蓝色为prepare阶段，黄色为accept阶段

![paxos](..\img\paxos&zookeeper\paxos.png)



### Paxos的工程实践

#### Chubby

##### 简介

面向松耦合分布式系统的锁服务，提供粗粒度的分布式锁服务（粗粒度：指锁的持有时间比较长，减少服务器压力），对外提供锁服务接口，chubby设计成一个锁服务而不是Paxos算法库的原因，基于以下几个点：

- 对上层应用程序的侵入性更小，在系统开发前期一般不会关注一致性问题，不会预留相关的接口，而在后期需要关注一致性问题时，通过服务接口调用的方式可以很快速集成，并且也不需要进行过多的改动
- 便于提供数据的发布和订阅
- 开发人员对于锁的接口更加熟悉
- 可以更便捷的构建可靠的服务，分布式一致性算法一般都要通过Quorum机制（过半机制，集群中半数以上的机器可用）来保证集群的可用性，提供锁服务省去了开发人员处理服务可用性的成本

##### 架构

![](..\img\paxos&zookeeper\chubby.png)





## Zab

Zab借鉴了paxos，是专门为zookeeper设计的支持崩溃恢复的原子广播协议

### 核心

所有事务请求必须经由全局唯一的服务器来协调处理，即Leader服务器，其余的服务器成为Follower服务器

### 消息广播

是一个简化的2PC过程

![ZAB-消息广播](..\img\paxos&zookeeper\ZAB-消息广播.png)



### 崩溃恢复

#### 已处理的消息不能丢

当集群中的leader崩溃时，集群进入崩溃恢复模式，其他follower中选出一个具有最大ZXID的机器（ZXID越大，意味着接收到的proposal最新，越可能包含完整的信息），此时这台机器升级为新的leader，其他机器成为新的follower。

新的leader与follower建立通信，将follower中没有的proposal发布过去，并紧跟一个COMMIT，因为这些proposal理应已经被处理了，最后所有设备都保持和leader一致的状态。



#### 被丢弃的消息不能再次出现

当新的leader重启再次假如集群时，他自身可能带有一些还没有来得及发出的proposal，那么他的ZXID肯定大于恢复时的ZXID，但其实他们是不需要的proposal，如何解决呢？

Zab 通过巧妙的设计 ZXID来实现这一目的。一个 zxid 是64位，高 32 是纪元（epoch）编号，每经过一次 leader 选举产生一个新的 leader，新 leader 会将 epoch 号 +1。低 32 位是消息计数器，每接收到一条消息这个值 +1，新 leader 选举后这个值重置为 0。这样设计的好处是旧的 leader 挂了后重启，它不会被选举为 leader，因为此时它的 zxid 肯定小于当前的新 leader。当旧的 leader 作为 follower 接入新的 leader 后，新的 leader 会让它将所有的拥有旧的 epoch 号的未被 COMMIT 的 proposal 清除。   



### 选举

(1)每个Server发出一个投票。由于是初始情况，ZK1和ZK2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时ZK1的投票为(1, 0)，ZK2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。 

(2)接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。 　

(3)处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行比较，规则如下：　　

- 优先检查ZXID。ZXID比较大的服务器优先作为Leader。 　　　
- 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。 　

对于ZK1而言，它的投票是(1, 0)，接收ZK2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时ZK2的myid最大，于是ZK2胜。ZK1更新自己的投票为(2, 0)，并将投票重新发送给ZK2。 　

(4)统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于ZK1、ZK2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出ZK2作为Leader。 

(5)改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。当新的Zookeeper节点ZK3启动时，发现已经有Leader了，不再选举，直接将直接的状态从LOOKING改为FOLLOWING。



### ZK实现简单的分布式锁

#### 排它锁-临时节点

创建临时节点的原因是当设备宕机时，出于安全考虑，需要让该设备让出持有的锁，临时节点的特性正好可以满足这个需求

1. **加锁**

   各客户端分别向ZK服务发送消息创建/lock节点，基于zk的机制，最先收到的创建节点消息会被执行，即加锁成功；而其他消息会返回失败，即加锁失败，随后创建这个节点的watch

2. **解锁**

   持有锁的客户端向ZK服务发送消息删除/lock节点，其他watch这个节点的客户端收到消息后可以继续尝试向ZK服务中创建节点进行加锁

#### 共享锁-临时顺序节点

1. **加锁**

   各客户端分别向ZK服务发送消息创建/lock/shared-[R/W]，R/W代表该节点是读请求还是写请求，临时顺序节点使得收到的每个消息会以先来后到的顺序在节点下创建数值越来越大的节点

2. **解锁**

   持有锁的客户端向ZK服务发送消息删除/lock/shared-[R/W]节点，其他节点收到watch通知后有如下的不同：

   对于读请求：若没有比自己序号小的子节点或所有比自己序号小的子节点都是读请求，那么表明自己已经成功获取到共享锁，同时开始执行读取逻辑，若有写请求，则需要等待。

   对于写请求：若自己不是序号最小的子节点，那么需要等待。

**优化**

上面的问题容易出现羊群效应，即一个节点被释放后，所有watch它的节点都被唤醒，带来短时间的极大流量，但是其实大部分节点的通知又是不必要的，优化办法如下：

读请求：注册比自己序号小的最后一个写请求节点watch

写请求：注册比自己序号小的最后一个节点watch



### ZK实现简单的分布式队列

1. **数据结构**

   在ZK Server中创建临时顺序节点，临时顺序节点使得收到的每个消息会以先来后到的顺序在节点下创建数值越来越大的节点

2. **入队**

   各客户端分别向ZK服务发送消息创建/queue/data临时顺序节点

3. **出队**

   将/queue下最小数值的节点删除







