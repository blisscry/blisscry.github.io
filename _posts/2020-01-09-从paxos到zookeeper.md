---

layout:     post
title:      从paxos到zookeeper
subtitle:   分布式
date:       2020-01-09
author:     skaleto
catalog: true
tags:
    - 分布式

---



## Paxos

paxos算法的基本流程如下图，蓝色为prepare阶段，黄色为accept阶段

![paxos](..\img\paxos&zookeeper\paxos.png)





## Chubby

##### 简介

面向松耦合分布式系统的锁服务，提供粗粒度的分布式锁服务（粗粒度：指锁的持有时间比较长，减少服务器压力），对外提供锁服务接口，chubby设计成一个锁服务而不是Paxos算法库的原因，基于以下几个点：

- 对上层应用程序的侵入性更小，在系统开发前期一般不会关注一致性问题，不会预留相关的接口，而在后期需要关注一致性问题时，通过服务接口调用的方式可以很快速集成，并且也不需要进行过多的改动
- 便于提供数据的发布和订阅
- 开发人员对于锁的接口更加熟悉
- 可以更便捷的构建可靠的服务，分布式一致性算法一般都要通过Quorum机制（过半机制，集群中半数以上的机器可用）来保证集群的可用性，提供锁服务省去了开发人员处理服务可用性的成本

##### 架构

![](..\img\paxos&zookeeper\chubby.png)

- cell中的的副本服务器（replica）通过paxos协议选举出master，一旦一个master在运行过程中，他会不断续租，使得一段时间内其他服务器不会成为Master，如果Master服务器发生故障，余下服务器会选举新的Master
- 只有Master服务器才能对数据进行写操作，其他服务器只能从Master上同步更新内容

##### 目录与文件

- 对外提供类似Unix文件系统的接口，路径表示如下：/ls/foo/foo，ls是所有节点共同的前缀，代表Lock Service
- 每个数据节点都分为持久节点和临时节点
- 每个数据节点都包含了少量的元数据信息，包括用于权限控制的访问控制列表（ACL）信息

##### 锁与锁序列器

chubby的任意一个数据节点都可以充当一个读写锁，包括排他锁和共享锁。如果一个客户端获得了锁，但是由于某些原因，它的响应延迟到达，那么chubby会认为他可能已经没有存活了，会将这个锁释放开，但实际情况下可能该客户端网络有一定延迟，在一段时间后又能正常响应了，这种情况下如果在这段时间内其他客户端获得了锁，改变了数据状态，此时该客户端再进行修改就会造成不一致，chubby解决上述的两种方式有锁延迟和锁序列器。

- 锁延迟，顾名思义，当客户端由于异常情况而释放锁时，chubby会为该锁保留一段时间，他能够有效减少网络闪断带来的影响，但显而易见的是，这种方案并不保险
- 锁序列器，客户端在请求锁时需要带上锁序列器，如果锁序列器没有通过检查，服务端会拒绝该客户端请求

##### 事件通知机制

客户端可以向chubby注册事件通知，当chubby内部有如下的事件发生时，chubby会通知对应的客户端

- 文件内容变更
- 节点删除，子节点新增、删除
- Master服务器转移

##### 缓存

在客户端有缓存，用来减少读请求的压力，缓存通过租期来保证一致性，缓存的生命周期和Master的租期密切相关





## Raft

Raft是Stanford提出的一种更容易理解的一致性算法，意在取代目前广为使用的Paxos算法，在Raft中，有如下三种类型的节点：

- **follower**：所有结点都以follower的状态开始。如果没收到leader消息则会变成candidate状态
- **candidate**：会向其他结点“拉选票”，如果得到大部分的票则成为leader。这个过程是Leader选举
- **leader**：所有对系统的修改都会先经过leader。每个修改都会写一条日志(log entry)。leader收到修改请求后的过程如下，这个过程叫做日志复制(Log Replication)

### Raft leader选举

1. 集群中没有leader时，将发起选举，但每个follower不是立即变成candidate，每个follower都有一个各自的election timeout，是150ms到300ms之间的随机数，election timeout决定了follower在成为candidate之前等待的时间。
2. 当某个follower A等待时间到达，且这段时间内没有收到其他的消息，那么它将转变成candicate，并且向集群中的其他机器发送vote message，如果接收到的设备还没有投票过，那么他们将投票给A，并且将自己的超时时间重置；
3. A收到超过半数的投票后，成为Leader，向所有节点发送心跳包和messgae
4. 每个节点收到心跳包后也会重置自己的超时时间，直到不再接收到新的心跳包并且超时时间到达，再次成为candidate
5. 假如有两个follower同时到达超时时间并变成candidate，那么两个节点得到的投票可能都不超过半数，那么他们将再次自旋，等待新一轮的leader选举

### 分区问题

- 假设有两个机房被网络隔开了，那么原本没有leader存在的机房会经过选举产生一个新的leader
- 此时假如这个新的leader接收到了来自客户端的请求，但是由于他不能收到来自多数的成功提交消息，那么这次请求将是uncommited状态
- 当网络再次连通时，原本leader上的消息已经比他更新了，因此被隔开机房中的节点都会恢复成follower并且回滚未提交的事务







## Zookeeper设计的支持崩溃恢复的原子广播协议



## Zab

Zab借鉴了paxos，是专门为zookeeper设计的支持崩溃恢复的原子广播协议

### 核心

所有事务请求必须经由全局唯一的服务器来协调处理，即Leader服务器，其余的服务器成为Follower服务器

### 消息广播

是一个简化的2PC过程

![ZAB-消息广播](..\img\paxos&zookeeper\ZAB-消息广播.png)



### 崩溃恢复

#### 已处理的消息不能丢

当集群中的leader崩溃时，集群进入崩溃恢复模式，其他follower中选出一个具有最大ZXID的机器（ZXID越大，意味着接收到的proposal最新，越可能包含完整的信息），此时这台机器升级为新的leader，其他机器成为新的follower。

新的leader与follower建立通信，将follower中没有的proposal发布过去，并紧跟一个COMMIT，因为这些proposal理应已经被处理了，最后所有设备都保持和leader一致的状态。



#### 被丢弃的消息不能再次出现

当新的leader重启再次假如集群时，他自身可能带有一些还没有来得及发出的proposal，那么他的ZXID肯定大于恢复时的ZXID，但其实他们是不需要的proposal，如何解决呢？

Zab 通过巧妙的设计 ZXID来实现这一目的。一个 zxid 是64位，高 32 是纪元（epoch）编号，每经过一次 leader 选举产生一个新的 leader，新 leader 会将 epoch 号 +1。低 32 位是消息计数器，每接收到一条消息这个值 +1，新 leader 选举后这个值重置为 0。这样设计的好处是旧的 leader 挂了后重启，它不会被选举为 leader，因为此时它的 zxid 肯定小于当前的新 leader。当旧的 leader 作为 follower 接入新的 leader 后，新的 leader 会让它将所有的拥有旧的 epoch 号的未被 COMMIT 的 proposal 清除。   



### 选举

(1)每个Server发出一个投票。由于是初始情况，ZK1和ZK2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时ZK1的投票为(1, 0)，ZK2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。 

(2)接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。 　

(3)处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行比较，规则如下：　　

- 优先检查ZXID。ZXID比较大的服务器优先作为Leader。 　　　
- 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。 　

对于ZK1而言，它的投票是(1, 0)，接收ZK2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时ZK2的myid最大，于是ZK2胜。ZK1更新自己的投票为(2, 0)，并将投票重新发送给ZK2。 　

(4)统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于ZK1、ZK2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出ZK2作为Leader。 

(5)改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。当新的Zookeeper节点ZK3启动时，发现已经有Leader了，不再选举，直接将直接的状态从LOOKING改为FOLLOWING。



### ZK实现简单的分布式锁

#### 排它锁-临时节点

创建临时节点的原因是当设备宕机时，出于安全考虑，需要让该设备让出持有的锁，临时节点的特性正好可以满足这个需求

1. **加锁**

   各客户端分别向ZK服务发送消息创建/lock节点，基于zk的机制，最先收到的创建节点消息会被执行，即加锁成功；而其他消息会返回失败，即加锁失败，随后创建这个节点的watch

2. **解锁**

   持有锁的客户端向ZK服务发送消息删除/lock节点，其他watch这个节点的客户端收到消息后可以继续尝试向ZK服务中创建节点进行加锁

#### 共享锁-临时顺序节点

1. **加锁**

   各客户端分别向ZK服务发送消息创建/lock/shared-[R/W]，R/W代表该节点是读请求还是写请求，临时顺序节点使得收到的每个消息会以先来后到的顺序在节点下创建数值越来越大的节点

2. **解锁**

   持有锁的客户端向ZK服务发送消息删除/lock/shared-[R/W]节点，其他节点收到watch通知后有如下的不同：

   对于读请求：若没有比自己序号小的子节点或所有比自己序号小的子节点都是读请求，那么表明自己已经成功获取到共享锁，同时开始执行读取逻辑，若有写请求，则需要等待。

   对于写请求：若自己不是序号最小的子节点，那么需要等待。

**优化**

上面的问题容易出现羊群效应，即一个节点被释放后，所有watch它的节点都被唤醒，带来短时间的极大流量，但是其实大部分节点的通知又是不必要的，优化办法如下：

读请求：注册比自己序号小的最后一个写请求节点watch

写请求：注册比自己序号小的最后一个节点watch



### ZK实现简单的分布式队列

1. **数据结构**

   在ZK Server中创建临时顺序节点，临时顺序节点使得收到的每个消息会以先来后到的顺序在节点下创建数值越来越大的节点

2. **入队**

   各客户端分别向ZK服务发送消息创建/queue/data临时顺序节点

3. **出队**

   将/queue下最小数值的节点删除



## Observer

Observer是zookeeper中的一种新型节点，在zookeeper3.3.0版本开始引入，他的实现原理和Follower基本一致，对于非事务请求，例如读请求，可以单独处理；对于事务请求，都会转发给Leader进行处理。并且Observer不参与任何形式的投票，包括proposal和leader选举。通常用于在不影响集群事务处理能力的前提下提升集群的非事务处理能力。

使用Observer模式的一个主要的理由就是对读请求进行扩展。通过增加更多的Observer，可以接收更多的请求的流量，却不会牺牲写操作的吞吐量。注意到写操作的吞吐量取决于quorum的Size。如果增加更多的Server进行投票，quorum会变大，这会降低写操作的吞吐量。然而增加Observer并不会完全没有损耗，每一个新的Observer在每提交一个事务后收到一条额外的消息，这就是前面提到的INFORM消息。这个损耗比起加入Follower来投票来说损耗更少。 

使用Observer的另一个原因是跨数据中心部署。把participant分散到多个数据中心可能会极大拖慢系统，因为数据中心之间的网络的延迟。使用Observer的话，更新操作都在一个单独的数据中心来处理，并发送到其他数据中心，让其他数据中心的client消费数据。

注意Observer的使用并无法完全消除数据中心之间的网络延迟，因为Observer不得不把更新请求转发到另一个数据中心的Leader，并处理INFORM消息，网络速度极慢的话也会有影响，它的优势是为本地读请求提供快速响应。





## 脑裂问题

假设有两个机房，分别都有3台设备，整个集群一共6台设备，当机房1和机房2中的通信网络由于某种原因（网络超时）断开之后，如果没有某种机制的保护，原先不存在Leader的机房2中就会选举产生一个新的Leader，当网络再次联通时，集群中就会出现两个Leader，这会带来严重的问题。

![brain_split](E:\skaleto.github.io\img\paxos&zookeeper\brain_split.png)

但假如我们规定，对于N个设备的集群，必须有大于N/2台设备参与选举才能产生新的Leader，这种机制叫过半机制，也是Zookeeper中的常用的Quorums机制。

此时再来看上图，网络断开后，机房2中的设备数量3=6/2，没有超过半数，因此机房2不能选举产生新的Leader，也就不会有脑裂问题了。

此外，还有其他几种方式用来避免出现脑裂的问题

- 添加冗余的心跳线，例如双心跳线
- 启动磁盘锁，正在服务的一方锁住共享磁盘，即使产生脑裂，另一方也抢不走共享磁盘资源
- 设置仲裁机制



ZooKeeper和Raft在一旦分区发生的情况下是牺牲了高可用来保证一致性，即CAP理论中的CP。但是在没有分区发生的情况下既能保证高可用又能保证一致性，所以更想说的是所谓的CAP二者取其一，并不是说该系统一直保持CA或者CP或者AP，而是一个会变化的过程。在没有分区出现的情况下，既可以保证C又可以保证A，在分区出现的情况下，那就需要从C和A中选择一样。ZooKeeper和Raft则都是选择了C。





### Zookeeper数据结构

#### B-tree

- 关系型数据库Oracle、SQL Server、MySQL和PostgreSQL都支持B-tree
- WiredTiger是MongoDB的默认存储引擎，也支持B-tree
- BoltDB：Go语言开发的B-tree存储引擎







## ETCD

etcd是一个高可用的分布式KV系统，可以用来实现各种分布式协同服务，基于Go语言实现了raft算法作为一致性协议，最初由CoreOS团队研发。

*etcd名称由来：etc是UNIX系统的配置文件目录，d代表distributed system*

- kubernetes使用etcd来做服务发现和配置信息管理
- openstack使用etcd来做配置关系和分布式锁
- ROOK使用etcd研发编排引擎

etcd不同于zookeeper需要将数据加载到内存，它使用bbolt引擎，可以处理几十GB的数据。

##### 数据结构

K-V结构，按照Key的字母序排列





## Zookeeper本地存储

### 序列化

序列化方案：Apache Jute，



### 











