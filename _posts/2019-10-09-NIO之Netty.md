---

layout:     post
title:      NIO之Netty
subtitle:   NIO之Netty
date:       2019-10-09
author:     skaleto
catalog: true
tags:
    - nio,netty

---

# 

[TOC]

https://www.cnblogs.com/crazymakercircle/p/10225159.html

https://www.cnblogs.com/crazymakercircle/p/10225159.html



## IO多路复用中Select的底层原理

*前提：每个socket中可以看做拥有一个等待队列，用来存放所有监听它的工作进程

### select

1. 将进程加入到它监听的所有socket的等待队列中，将进程从CPU工作队列中移除，进入阻塞状态
2. 当有一个socket有数据返回时，中断程序把进程从所有的socket等待队列中移除，并把进程重新加入CPU工作队列，进入就绪状态
3. 进程遍历所有socket的状态，知道哪些socket可以读取数据，然后去读取数据
4. 操作完成后，再重复第一步

特点：

- 底层限制了一个select最多监控1024个socket
- 因为需要重复执行所有socket等待队列的加入移除操作，比较耗性能

### poll

与select的原理差不多，但去除了1024个socket的限制

### epoll

进程调用epoll监控多个socket时，会在底层创建一个eventpoll对象，包含了一个就绪队列

1. 进程调用epoll后，epoll将这个进程加入eventpoll对象的等待队列中
2. 将eventpoll对象加入所有socket的等待队列，并使进程阻塞
3. 当有一个socket有数据返回时，中断程序将该socket加入到eventpoll对象的就绪队列中，并把eventpoll中等待的进程唤醒
4. 进程唤醒后直接从就绪队列中获取socket读取数据
5. 操作完成后，再重复第一步，但不需要执行第二步，因为eventpoll对象始终在所有监听的socket中

特点：

- 每个socket的等待队列不会发生频繁的加入移除操作，只会被加入一次eventpoll对象



## Reactor模型

```
反应器模式(Reactor)和观察者模式的联系和区别：
都是当一个主体发生变化时，所有依属体都得到通知
观察者模式与单个时间源关联，而反应器模式与多个事件源关联
```

https://www.cnblogs.com/crazymakercircle/p/9904544.html

实际上的Reactor模式，是基于Java NIO的，在他的基础上，抽象出来两个组件——Reactor和Handler两个组件：

（1）Reactor：负责响应IO事件，当检测到一个新的事件，将其发送给相应的Handler去处理；新的事件包含连接建立就绪、读就绪、写就绪等。

（2）Handler:将自身（handler）与事件绑定，负责事件的处理，完成channel的读入，完成处理业务逻辑后，负责将结果写出channel。



### 单线程的Reactor模型

![wpsC334.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181022232217633-124484857.jpg)

![wpsC345.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181022232218623-1679192697.jpg)

### 多线程的Reactor模型

单线程模式Reactor的缺点：

1、 当其中某个 handler 阻塞时， 会导致其他所有的 client 的 handler 都得不到执行， 并且更严重的是， handler 的阻塞也会导致整个服务不能接收新的 client 请求(因为 acceptor 也被阻塞了)。 因为有这么多的缺陷， 因此单线程Reactor 模型用的比较少。这种单线程模型不能充分利用多核资源，所以实际使用的不多。

2、因此，单线程模型仅仅适用于handler 中业务处理组件能快速完成的场景。



在线程Reactor模式基础上，做如下改进：

（1）将Handler处理器的执行放入线程池，多线程进行业务处理。

（2）而对于Reactor而言，可以仍为单个线程。如果服务器为多核的CPU，为充分利用系统资源，可以将Reactor拆分为两个线程。

![image](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181022232219420-1734756772.png)

![wpsC376.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181022232220631-1867817712.jpg)



![img](https://images0.cnblogs.com/blog2015/434101/201503/112151380898648.jpg)

mainReactor负责监听连接，accept连接给subReactor处理，为什么要单独分一个Reactor来处理监听呢？因为像TCP这样需要经过3次握手才能建立连接，这个建立连接的过程也是要耗时间和资源的，单独分一个Reactor来处理，可以提高性能。



#### Reactor的优缺点

##### 优点

1）响应快，不必为单个同步时间所阻塞，虽然Reactor本身依然是同步的；

2）编程相对简单，可以最大程度的避免复杂的多线程及同步问题，并且避免了多线程/进程的切换开销；

3）可扩展性，可以方便的通过增加Reactor实例个数来充分利用CPU资源；

4）可复用性，reactor框架本身与具体事件处理逻辑无关，具有很高的复用性；

##### 缺点

1）相比传统的简单模型，Reactor增加了一定的复杂性，因而有一定的门槛，并且不易于调试。

2）Reactor模式需要底层的Synchronous Event Demultiplexer支持，比如Java中的Selector支持，操作系统的select系统调用支持，如果要自己实现Synchronous Event Demultiplexer可能不会有那么高效。

3） Reactor模式在IO读写数据时还是在同一个线程中实现的，即使使用多个Reactor机制的情况下，那些共享一个Reactor的Channel如果出现一个长时间的数据读写，会影响这个Reactor中其他Channel的相应时间，比如在大文件传输时，IO操作就会影响其他Client的相应时间，因而对这种操作，使用传统的Thread-Per-Connection或许是一个更好的选择，或则此时使用改进版的Reactor模式如Proactor模式。



##### Reactor模式的组成部分

![wps560D.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181025005832332-788135728.png)

Reactor模式可以被分为三大步骤：注册、轮询、分发

![wps561E.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181025005832917-738562887.png)







## Netty

### 简介和优点

Netty的目标之一，是要使开发可以做到快速和轻松，支持TCP/UDP、FTP、SMTP、HTTP等多种协议；目标之二是要做到高性能、高可扩展性

Netty提供异步的、事件驱动的网络应用程序框架和工具，相比JDK原生NIO，Netty提供了相对十分简单易用的API，非常适合网络编程。Netty是完全基于NIO实现的，所以Netty是异步的。

作为一个异步NIO框架，Netty的所有IO操作都是异步非阻塞的，通过Future-Listener机制，用户可以方便的主动获取或者通过通知机制获得IO操作结果。

Netty的优点可以总结如下

1、API使用简单，开发门槛低；

2、功能强大，预置了多种编解码功能，支持多种主流协议；

3、定制能力强，可以通过ChannelHandler对通信框架进行灵活地扩展；

4、性能高，通过与其他业界主流的NIO框架对比，Netty的综合性能最优；

5、成熟、稳定，Netty修复了已经发现的所有JDK NIO BUG，业务开发人员不需要再为NIO的BUG而烦恼；

6、社区活跃，版本迭代周期短，发现的BUG可以被及时修复，同时，更多的新功能会加入；



Netty中的Channel系列类型，对应于经典Reactor模型中的client， 封装了用户的通讯连接。

Netty中的EventLoop系列类型，对应于经典Reactor模型中的Reactor，完成Channel的注册、轮询、分发。

Netty中的Handler系列类型，对应于经典Reactor模型中的Handler，不过Netty中的Handler设计得更加的高级和巧妙，使用了Pipeline模式。

![wps562E.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181025005656468-925698953.png)



### Netty中的Channel通道组件

Netty中对常见的各种协议都有对应的阻塞或非阻塞的通道组件与之对应

NioSocketChannel： 代表异步的客户端 TCP Socket 连接.

NioServerSocketChannel： 异步的服务器端 TCP Socket 连接.

NioDatagramChannel： 异步的 UDP 连接

NioSctpChannel： 异步的客户端 Sctp 连接.

NioSctpServerChannel： 异步的 Sctp 服务器端连接.

OioSocketChannel： 同步的客户端 TCP Socket 连接.

OioServerSocketChannel： 同步的服务器端 TCP Socket 连接.

OioDatagramChannel： 同步的 UDP 连接

OioSctpChannel： 同步的 Sctp 服务器端连接.

OioSctpServerChannel： 同步的客户端 TCP Socket 连接.





### NioEventLoop

我们知道在Reactor模式中，存在一个反应器的角色，用来进行不断的轮询和查询事件，并将事件进行分发，在Netty中反应器有多个实现，对应于NioSocketChannel的反应器就是NioEventLoop（因为TCP协议下的网络通信是最为常见的，所以拿他举例）

#### 主要结构

NioEventLoop对应于reactor模式下的Reactor组件，在reactor模式下，我们首先将channel注册到selector上，并attach一个对象，用来处理连接事件，那么在netty中是如何的呢？

下图是NioEventLoop的继承关系和部分代码

![wps565F.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181025005657829-91055201.png)

```java
public final class NioEventLoop extends SingleThreadEventLoop {
    //继承了netty自己实现的一个线程池
    ...
    /**
     * The NIO {@link Selector}.
     * 这里有两个selector成员变量，与我们自己实现的Reactor类似
     */
    private Selector selector;
    private Selector unwrappedSelector;
    private SelectedSelectionKeySet selectedKeys;
    ...
}
```







#### Reactor三部曲之注册

Channel向EventLoop注册的过程，是在启动时进行的。注册的入口代码，在启动类AbstractBootstrap.initAndRegister 方法中。

注册入口代码如下：

```java
    final ChannelFuture initAndRegister() {
        Channel channel = null;
        try {
            //TODO 此处反射调用了channel的无参构造方法
            channel = channelFactory.newChannel();
            //不同channel有不同的初始化方法
            init(channel);
        } catch (Throwable t) {
            ...
        }

        ChannelFuture regFuture = config().group().register(channel);
        if (regFuture.cause() != null) {
            if (channel.isRegistered()) {
                channel.close();
            } else {
                channel.unsafe().closeForcibly();
            }
        }

        return regFuture;
    }
```

![wps56A1.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181025005659771-650248316.png)



注册阶段的主要代码在AbstractChannel中

```java
public abstract class AbstractChannel extends DefaultAttributeMap implements Channel {
    //...
    protected abstract class AbstractUnsafe implements Unsafe {
        //...

        @Override
        public final void register(EventLoop eventLoop, final ChannelPromise promise) {
            //...
            //传入的eventLoop根据不同的调用者有不同的实现，使用传入的eventLoop来进行下面的工作
            AbstractChannel.this.eventLoop = eventLoop;

            if (eventLoop.inEventLoop()) {
                register0(promise);
            } else {
                try {
                    //因为eventloop继承自线程池，所以这个地方调用线程池的execute方法开启一个线程
                    eventLoop.execute(new Runnable() {
                        @Override
                        public void run() {
                            register0(promise);
                        }
                    });
                } catch (Throwable t) {
                    //...
                }
            }
        }

        private void register0(ChannelPromise promise) {
            try {
                //...
                boolean firstRegistration = neverRegistered;
                //真正注册的地方
                doRegister();
                neverRegistered = false;
                registered = true;

                pipeline.invokeHandlerAddedIfNeeded();

                safeSetSuccess(promise);
                pipeline.fireChannelRegistered();

                if (isActive()) {
                    if (firstRegistration) {
                        pipeline.fireChannelActive();
                    } else if (config().isAutoRead()) {
                        beginRead();
                    }
                }
            } catch (Throwable t) {
                //...
            }
        }
             
}
```

```java
public abstract class AbstractNioChannel extends AbstractChannel {
	@Override
    protected void doRegister() throws Exception {
        boolean selected = false;
        
        //死循环，要么注册成功，要么抛出异常
        for (;;) {
            try {
                //获得在构造函数中初始化的nio channel并注册到selector上
                //这里的register在注册的时候带上的attachment是this，也就是AbstractNioChannel本身
                selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this);
                return;
            } catch (CancelledKeyException e) {
                if (!selected) {
                    //假如捕获到cancelled异常，此时这个key可能还存在，因为没有人去调用过select方法，所以这里强制去select一次
                    eventLoop().selectNow();
                    selected = true;
                } else {
                    throw e;
                }
            }
        }
    }
    
    //javaChannel方法返回一个java nio的SelectableChannel，可以看到netty的底层就是用的java nio
    protected SelectableChannel javaChannel() {
        return ch;
    }
}
```

从上面的过程中我们可以理出这样一个过程，在初始化的时候，首先传入一个eventloop，在这个eventloop中开启了一个线程去进行注册的操作，在eventloop中获得了一个java nio底层的channel，并将它注册到eventloop内部的一个selector上，从而将channel与eventloop关联上了



#### Reactor三部曲之轮询

![wps56B1.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181025005700385-1798901391.png)

上面从源码中看到，NioEventLoop继承自一个自定义的单线程线程池，那么这个线程池中的线程在什么时候启动呢？

在上面的AbstractChannel的register中，注册操作中有一步调用execute，对应执行SingleThreadExecutor中的execute方法

```java
            if (eventLoop.inEventLoop()) {
                register0(promise);
            } else {
                try {
                    //这个地方调用线程池的execute方法开启一个线程
                    eventLoop.execute(new Runnable() {
                        @Override
                        public void run() {
                            register0(promise);
                        }
                    });
                } catch (Throwable t) {
                    //...
                }
            }  
```

上面只是开启一个线程池，具体轮询的逻辑在NioEventLoop#run()中

```java
protected void run(){
        for (;;) {
            try {
                try {
                    //这里有一个策略模式
                    switch (selectStrategy.calculateStrategy(selectNowSupplier, hasTasks())) {
                    case SelectStrategy.CONTINUE:
                        continue;
                    case SelectStrategy.BUSY_WAIT:
                    case SelectStrategy.SELECT:
                        select(wakenUp.getAndSet(false));
                        if (wakenUp.get()) {
                            selector.wakeup();
                        }
                    default:
                    }
                } catch (IOException e) {
                    //...
                }

                //这里会根据io比例来设置任务执行的超时时间
                final int ioRatio = this.ioRatio;
                if (ioRatio == 100) {
                    try {
                        //处理被选中的key
                        processSelectedKeys();
                    } finally {
                        //执行任务
                        runAllTasks();
                    }
                } else {
                    final long ioStartTime = System.nanoTime();
                    try {
                        processSelectedKeys();
                    } finally {
                        final long ioTime = System.nanoTime() - ioStartTime;
                        runAllTasks(ioTime * (100 - ioRatio) / ioRatio);
                    }
                }
            } catch (Throwable t) {
            }
        }
}
```

```java
private void processSelectedKeys() {
        if (selectedKeys != null) {
            processSelectedKeysOptimized();
        } else {
            processSelectedKeysPlain(selector.selectedKeys());
        }
}
    
private void processSelectedKeysOptimized() {
    	//为每个事件调用processSelectedKey来处理
        for (int i = 0; i < selectedKeys.size; ++i) {
            final SelectionKey k = selectedKeys.keys[i];
            selectedKeys.keys[i] = null;
            //注意看到这里拿到的k.attachment就是上面在注册时候放进去的AbstractNioChannel对象
            final Object a = k.attachment();
            //检查是否为AbstractNioChannel的实例
            if (a instanceof AbstractNioChannel) {
                //处理这个key的事件
                processSelectedKey(k, (AbstractNioChannel) a);
            } else {
                @SuppressWarnings("unchecked")
                NioTask<SelectableChannel> task = (NioTask<SelectableChannel>) a;
                processSelectedKey(k, task);
            }

        }
}
```

```java
private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {
    	//NioUnsafe是AbstractNioChannel的一个内部类，负责用来对底层的channel做处理，例如连接、读、写等
        final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();
        if (!k.isValid()) {
            //...
        }

        try {
            int readyOps = k.readyOps();
            //连接就绪事件
            if ((readyOps & SelectionKey.OP_CONNECT) != 0) {
                
                int ops = k.interestOps();
                ops &= ~SelectionKey.OP_CONNECT;
                k.interestOps(ops);

                unsafe.finishConnect();
            }

            //写就绪事件
            if ((readyOps & SelectionKey.OP_WRITE) != 0) {
                ch.unsafe().forceFlush();
            }
            
            //读就绪事件
            if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {
                unsafe.read();
            }
        } catch (CancelledKeyException ignored) {
            unsafe.close(unsafe.voidPromise());
        }
}
```

processSelectedKey 中处理了三个事件, 分别是:

OP_READ, 可读事件, 即 Channel 中收到了新数据可供上层读取.

OP_WRITE, 可写事件, 即上层可以向 Channel 写入数据.

OP_CONNECT, 连接建立事件, 即 TCP 连接已经建立, Channel 处于 active 状态.



#### Reactor三部曲之分发

在AbstractNioByteChannel.NioByteUnsafe.read()负责将读到的数据分发给对应的handler

```java
public final void read() {
		//...
    ByteBuf byteBuf = null;
    boolean close = false;
    try {
        do {
          	//分配byteBuf
            byteBuf = allocHandle.allocate(allocator);
          	//读取数据
            allocHandle.lastBytesRead(doReadBytes(byteBuf));
            if (allocHandle.lastBytesRead() <= 0) {
                byteBuf.release();
              	//...
                break;
            }
            allocHandle.incMessagesRead(1);
            readPending = false;
          	//通过pipeline发送给需要执行的handler（pipeline在后面介绍）
            pipeline.fireChannelRead(byteBuf);
            byteBuf = null;
        } while (allocHandle.continueReading());
        allocHandle.readComplete();
        pipeline.fireChannelReadComplete();
      	//...
    } catch (Throwable t) {
    } finally {
    }
}
```



### Handler

![wps133D.tmp](https://img2018.cnblogs.com/blog/1485398/201810/1485398-20181025231828060-72968455.png)



Netty的ChannelHandler，主要分为两种，ChannelInboundHandler和ChannelOutboundHandler，对应入站和出站的两种操作（入站，指从Java NIO Channel传递数据到Netty Channel中，出站则反之），这两个Handler是两个接口，有各自的默认实现ChannelInboundHandlerAdapter和ChannelOutboundHandlerAdapter



### PipeLine

上面在说到netty中的分发时，我们看到有一个方法调用pipeline.fireChannelRead(byteBuf);

反应器Reactor和Channel之间是一对多的关系，但是Channel和Handler之间是多对多的关系，在Netty中，一个通道有一个PipeLine用来管理所有的Handler，称为ChannelPipeline，其中通过一个双向链表实现了一个数据结构，每个Handler作为双向链表的节点，数据在管道内从头向尾（入站事件）或从尾向头（出站事件）流动

一次事件假如是入站事件，那么它将只能在Inbound类型的Handler上流转，反之出站事件只能在Outbound类型的Handler上流转



### Netty Bootstrap

Netty提供了一个Bootstrap类来让开发者快速将Netty需要的各个组件组装起来，完成Netty的初始化。对于客户端和服务器端，分别有Bootstrap和ServerBootstrap两种

![image-20191031001051778](../img/netty/post-bg-unix-linux.png)

#### 父子通道

以socket为例，在服务端会有一个连接监听类型的socket负责监听和与成千上万个传输数据类型的socket建立连接，而Netty中NioServerSocketChannel底层就是封装了一种连接监听类型的socket，而NioSocketChannel则是封装了另一种，换而言之，两者之间一个是接受，一个是传输，称为父子通道。

#### EventLoopGroup线程组

前面在介绍Reactor模式的多线程场景下，一般会有一个subReactor专门用来做连接的监听和接受，另一个subReactore用来专门做IO事件的处理。Netty中有两个线程组，分别进行上面的两种操作



#### Bootstrap的启动流程

##### Step1 创建reactor线程组

```java
//boss线程组，负责连接监听和建立，一般设置为1
EventLoopGroup bossLoopGroup=new NioEventLoopGroup(1);
//worker线程组，负责进行IO事件和Handler的处理，线程数未制定时，默认初始化的线程数量并不是0，而是CPU处理器个数的两倍
EventLoopGroup workerLoopGroup=new NioEventLoopGroup();

ServerBootstrap bootstrap=new ServerBootstrap();
//将两个线程组放入bootstrap中
bootstrap.group(bossLoopGroup,workerLoopGroup);
```

这个地方也不是一定要创建两个线程组，当只用一个线程组的时候，Netty会认为这个线程组既可以用来做boss的功能也可以做worker的工作，但这样的话，监听和接受连接的线程可能被进行耗时IO操作的工作线程所阻塞，导致性能降低

```java
public ServerBootstrap group(EventLoopGroup group) {
    //单线程组的方法，其实是把这个线程组既当做boss线程也当做worker线程
    return group(group, group);
}

public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) {
    super.group(parentGroup);
  	//...
    this.childGroup = childGroup;
    return this;
}
```

##### Step2 设置通道IO类型

将通道类型设置为Nio类型

```java
bootstrap.channel(NioServerSocketChannel.class);
```

```java
public B channel(Class<? extends C> channelClass) {
    return channelFactory(new ReflectiveChannelFactory<C>(
            ObjectUtil.checkNotNull(channelClass, "channelClass")
    ));
}

public B channelFactory(io.netty.channel.ChannelFactory<? extends C> channelFactory) {
    return channelFactory((ChannelFactory<C>) channelFactory);
}
```

##### Step3 设置监听端口

```java
bootstrap.localAddress(9999);
```

localAddress方法有下面多种

```java
public B localAddress(SocketAddress localAddress) {
    this.localAddress = localAddress;
    return self();
}

public B localAddress(int inetPort) {
    return localAddress(new InetSocketAddress(inetPort));
}

public B localAddress(String inetHost, int inetPort) {
    return localAddress(SocketUtils.socketAddress(inetHost, inetPort));
}

public B localAddress(InetAddress inetHost, int inetPort) {
    return localAddress(new InetSocketAddress(inetHost, inetPort));
}
```

##### Step4 设置通道的配置选项

这里的option配置的是父通道的选项，如果要配置子通道的选项，需要用childOption

```java
bootstrap.option(ChannelOption.SO_KEEPALIVE,true);
bootstrap.option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT);
```

ChannelOption有如下几种：

- SO_KEEPALIVE，TCP参数，用来配置TCP协议的心跳机制，netty中默认关闭。开启的时候，TCP会主动检测空闲连接的有效性，默认心跳间隔是7200s
- SO_RCVBUF,SO_SNDBUF，TCP参数，用来配置TCP内核中接收区和发送区的缓冲区大小
- TCP_NODELAY，TCP参数，默认True，表示立即发送数据，这样可以提高实时性（设置为False的情况主要用于Nagle算法，即算法会将多个小的数据包合并成一个大包，从而减少发送数据包的数量）
- TCP_CORK，该选项与NODELAY相反，是需要等到发送的数据量最大的时候，一次性发送数据，适用于文件传输。
- SO_REUSEADDR，TCP参数，默认false，设为true时启用地址复用
- SO_LINGER，TCP参数，表示关闭socket的延迟时间，默认-1，即socket.close立即返回，但底层会将缓冲区数据全部发送出去（但不一定保证完全发送完）；0表示方法立即返回但放弃发送缓冲区；其他数值表示阻塞知道延迟时间或数据发送完毕
- SO_BACKLOG，TCP参数，表示服务器端接收连接的队列长度，如果队列已满，客户端的连接将被拒绝。此长度在Windows中为200，在其他操作系统中为128
- SO_BROADCAST，TCP参数，设置为广播模式
- ALLOCATOR，在Netty4.x版本中，重用缓冲区

##### Step5 装配子通道的Pipeline流水线

每一个通道的子通道都有一条ChannelPipeline与之对应，底层实现是一个双向链表

```java
//这里是为子通道装配流水线的操作
bootstrap.childHandler(new ChannelInitializer<SocketChannel>() {
    //有连接到达的时候会创建一个通道的子通道
    protected void initChannel(SocketChannel ch) throws Exception {
        //流水线管理子通道中的Handler业务处理器
        //向子通道流水线添加一个Handler业务处理器
        ch.pipeline().addLast(new NettyDiscardHandler());
    }
});
```

这里的childHandler仅仅是为子通道装配流水线的方法，如果需要为父通道装配定制的handler，可以调用bootstrap.handler()方法。（但是父通道往往只用来做连接的监听和创建，一般不需要额外配置）

##### Step6 绑定并监听端口

```java
//这里的bind是一个sync方法，阻塞直到bind方法完成
ChannelFuture channelFuture=bootstrap.bind().sync();
```

我们前面了解到Netty的所有事件都是异步的，都会返回一个ChannelFuture

至此，一个Netty服务器就启动完成了！



有了开启自然也有关闭，那么我们怎么配置服务器的关闭呢

```java
//阻塞直到通道关闭
ChannelFuture close = channelFuture.channel().closeFuture();
close.sync();
//分别将工作线程和boss线程关闭
workerLoopGroup.shutdownGracefully();
bossLoopGroup.shutdownGracefully();
```





### 详解Channel通道

在Netty中，通道是核心之一，负责与对端进行网络通信，写入数据到对端，也可以从对端读取数据

```java
protected AbstractChannel(Channel parent) {
    this.parent = parent;
    id = newId();
  	//unsafe中封装了java底层的socket操作，作为连接netty和java底层nio的桥梁（可以看做是完成实际IO操作的一个组件）
    unsafe = newUnsafe();
  	//每个channel都会对应一个pipeline
    pipeline = newChannelPipeline();
}
```

#### EmbeddedChannel嵌入式通道

Netty提供了一个EmbeddedChannel组件用来模拟入站和出站的操作，底层进行实际的传输，可以用来进行单元测试，有以下的几个方法可以使用：

- writeInbound，模拟入站事件，模拟通道接收到数据，将被入站处理器处理
- readInbound
- writeOutbound，模拟出站事件，将被出站处理器处理
- readOutbound
- finish



### 详解Handler业务处理器

Handler处理器主要负责的是将来自通道的数据包解码、进行业务处理、结果数据包的编码、写入通道

#### ChannelInboundHandler

此Handler负责进行数据包的解码和业务处理，有下面几个方法：

- channelRegistered，当通道注册完毕时，Netty中通过fireChannelRegistered发布注册事件
- channelActive，当通道激活时，Netty中通过fireChannelActive发布通道激活事件
- channelRead，当通道中有数据可读时，Netty中通过fireChannelRead发布通道可读事件
- channelReadComplete，通道缓冲区读取完毕事件
- channelInactive，通道连接被断开或不可用事件
- exceptionCaught，发生异常事件

#### ChannelOutboundHandler

出站操作是通过上层Netty通道去操作底层Java IO通道，此Handler负责进行结果数据包的编码和写入通道的操作

- bind，完成底层java IO通道的IP地址绑定
- connect
- write
- flush
- read
- disConnect
- close

#### ChannelInitializer通道初始化处理器

每一个channel都会绑定一个pipeline，而pipeline中依次存放了Handler，那么如何把Handler放入pipeline中呢，再来看看相关的代码。

```java
//这里是为子通道装配流水线的操作
bootstrap.childHandler(new ChannelInitializer<SocketChannel>() {
    //有连接到达的时候会创建一个通道的子通道
    protected void initChannel(SocketChannel ch) throws Exception {
        //流水线管理子通道中的Handler业务处理器
        //向子通道流水线添加一个Handler业务处理器
        ch.pipeline().addLast(new NettyDiscardHandler());
    }
});
```

addLast的入参为ChannelHandler的实现类，在Netty中有一个ChannelInboundHandlerAdapter，可以通过集成这个Adapter来编写自己的实现类



### 详解Pipeline流水线

Pipeline使用了一种设计模式：责任链模式

在上面的内容中，我们了解到，Netty每个channel都有一个pipeline，而pipeline底层是一个双向链表的结构，按顺序加入到pipeline中的入站处理器，将在入站时从头开始向后执行，而依次加入的出站处理器，将在出站时从后开始向前执行，并且入站只会触发入站处理器，相反出站只会触发出站处理器。

![img](https://img2018.cnblogs.com/blog/1547769/201901/1547769-20190101094549770-818535519.png)

从上图中可以看到，存放在pipeline中的不直接是handler对象，而是包装着它的一个ChannelHandlerContext

#### handler处理器热插拔

ChannelPipeline接口提供了一系列方法可以支持用户向其中增加和删除handler处理器

![image-20191104234537526](../img/netty/image-20191104234537526.png)

有一个特殊的处理器，ChannelInitializer，没错事实上他也是放在pipeline中执行的，当他完成初始化后，会将自身从pipeline中删去，以保证一条pipeline只会初始化一次



### ByteBuf缓冲区

java NIO中提供了ByteBuf用来操作缓冲区，而Netty中也有一个ByteBuf用来代替NIO，相比较而言，Netty的ByteBuf有如下几个优点：

- Pooling，池化，在Netty4.x版本开始，支持通过一个Buffer对象池用来存放哪些可能会被频繁创建和使用的缓存对象，从而提高资源利用率。Pooled(误，不存在Pooled这个类)和Unpooled两个静态类可以用来创建池化和非池化的ByteBuf
- 复合缓冲类型，支持零复制
- 不需要调用flip方法区切换读写模式，ByteBuf使用writerIndex和readerIndex来管理写入和读取的索引
- 扩展性好
- 可以自定义缓冲区类型
- 读取和写入索引分开
- 方法的链式调用
- 可以进行引用计数，方便重复使用



ByteBuf内部是一个字节数据，逻辑上来说可以分为废弃、可读、可写、可扩容部分

readerIndex用来指示可以读的初始位置，writerIndex用来指示可以写的初始位置，而capacity包括了废弃、可读和可写的部分，当容量不够时会触发扩容，最大扩容大小为maxCapacity

![image-20191105000339559](../img/netty/image-20191105000343212.png)

创建ByteBuf的方式可以使用PoolByteBufAllocator和UnpooledByteBufAllocator，在Netty4.0中，默认的Allocator分配器是UnpooledByteBufAllocator，而在Netty4.1中，由于池化技术的越来越成熟，默认的Allocator分配器则是PooledByteBufAllcator，创建方式有如下几种：

```java
//创建buffer的最快方法，内部与UnpooledByteBufAllocator.DEFAULT.heapBuffer()一致
ByteBuf buf = Unpooled.buffer();
//创建一个初始容量10，最大扩容容量100的buffer
ByteBuf buf2= ByteBufAllocator.DEFAULT.buffer(10,100);
//创建默认初始容量为256，最大扩容容量为Integer.MAX_VALUE的buffer
ByteBuf buf3=ByteBufAllocator.DEFAULT.buffer();
//创建基于java堆结构的非池化buffer
ByteBuf buf4= UnpooledByteBufAllocator.DEFAULT.heapBuffer();
//创建基于操作系统直接内存的池化buffer
ByteBuf buf5= PooledByteBufAllocator.DEFAULT.directBuffer();
```



#### 引用计数

在JVM的GC算法中，有一种引用计数法，即标记对象当前有多少引用存在，用来判断对象是否需要被GC回收，而Netty中也使用了引用计数法用来检查Buffer是否需要被回收，buffer有retain和release两个方法分别用来进行计数+1和-1的操作，简而言之，理论上来说，retain和release需要成对使用（而且JVM中有循环引用的问题，Netty中怎么解决？）

当引用计数为0时，Netty会对ByteBuf进行回收，分为如下两种情况：

- 池化的ByteBuf，将被放入可以重新分配的ByteBuf池子等待下一次分配
- 非池化的ByteBuf，如果是堆结构Buffer（存放在JVM的堆中，但在socket IO时，需要一次内存复制操作将数据从JVM拷贝至外部内存）将被JVM回收，如果是Direct类型结构的Buffer（存放在外部内存中，对于socket等IO操作有很好的性能）将调用本地方法释放外部内存



#### ByteBuf缓冲区的类型

上面说到ByteBuf有存放在JVM堆中，也有存放在外部直接内存中的buffer，下面是各种buffer类型的对比

| 类型            | 说明                                                         | 优点                                                         | 缺点                                                         |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Heap ByteBuf    | 存储在JVM的堆区，底层结构为Java数组                          | 在不使用池化的情况下，可以提供快速的分配和释放               | 由于存放在堆区，因此在底层IO传输时需要将数据从堆拷贝至直接缓冲区，再写入通道；并且受到堆大小的限制 |
| Direct ByteBuf  | 存储在操作系统的物理内存中，创建和销毁代价十分高昂，因此尽量分配在池中，提高buffer的利用率 | 存放在直接内存里，在底层IO传输时不需要额外的拷贝；并且不受堆大小的限制 | 需要使用native方法来分配和释放空间；并且数据需要在java中处理时，还需要将数据拷贝至堆上 |
| CompositeBuffer | 多个缓冲区的组合表示，即可以将多个不同类型的Buffer组合起来（例如HTTP请求中的Head与Body分开放在两个Buffer中处理，在传输的时候将Buffer组合起来传输） | 方便一次操作多个缓冲区实例                                   |                                                              |



#### ByteBuf的自动释放

ByteBuf在一个流程中被创建并处理后，必然会有一个地方将申请到的Buffer释放掉，那么它是在哪里释放的呢，在我们写的demo中，并没有直接编写buffer释放的代码，那么它会不会出问题呢？

答案是不会，在上面关于Pipeline的图中，我们看到，在双向链表的头部和尾部，有一个Head和Tail，事实上，在一个Pipeline被创建后，将在前后默认加上头和尾的处理器，他们是一个HeadHandler和TailHandler。TailHandler中包含了释放入站Buffer的操作，而HeadHandler中包含了释放出站Buffer的操作。



#### ByteBuf浅层复制

从入站或出站处理器中得到的数据，不一定是我们完全需要的，或者我们只需要其中的某些部分，那么可以使用ByteBuf的浅层复制。

- slice()，slice(int index, int length)，slice操作只是将原ByteBuf中的可读的部分做一次浅复制（浅复制是指并不是真正的复制一份数据），并且slice得到的ByteBuf不可写入
- duplicate()，duplicate操作则是将原ByteBuf的所有部分做一次浅复制

由于浅复制不是真正的复制数据，因此当源ByteBuf release后，如果引用计数为0了，那么对应它的浅复制也不能访问了





### Netty Decoder与Encoder

在Netty中，我们从通道中取出的ByteBuf是二进制数据，我们需要将其解码成Java的POJO对象，同样的在写入通道的时候，也要将POJO对象编码为ByteBuf二进制数据，而Netty已经提供了对应的解码和编码器。

#### ByteToMessageDecoder

ByteToMessageDecoder是一个抽象类，继承自ChannelInboundHandlerAdapter，也就是说，这个解码器也是一个入站处理器，具体的解码过程需要由子类来实现。

```java
protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) throws Exception {}
```

可以看到，需要实现的decode方法输入是ByteBuf，输出是List<Object>，通过ByteBuf提供的方法，我们可以自己实现从in中读取对应数量的字节，转换为字符串、数字等其他类型，再将之放入out数组中的操作。

但是在实际情况下，假如需要传输的数据量比较大，不可避免的我们需要将数据拆开分成多次发送，但是发送端发出去的包会在传输过程中进行多次的拆分和组装，最终可能出现发送端发出了四个包，而接收端只接收到了三个包的情况。（例如发送端依次发出AA,BB,CC,DD四个字符串，但是传输过程中被组装成了AAB,BCC,DD这样三个包，最终如果按照接收到的包来解析的话，得到的结果会与发送端大不相同）

为了解决这种问题，可以使用Netty提供的ReplayingDecoder。

```java
public abstract class ReplayingDecoder<S> extends ByteToMessageDecoder {
    private final ReplayingDecoderByteBuf replayable = new ReplayingDecoderByteBuf();
  	//保存当前处理状态的泛型state
    private S state;
  	//读断点指针，当可读数据不够时，ReplayingDecoder会把读指针的值还原到之前checkpoint的位置
    private int checkpoint = -1;
    
    protected ReplayingDecoder() {
        this(null);
    }
  
    protected ReplayingDecoder(S initialState) {
        state = initialState;
    }
}
```

我们可以使用ReplayingDecoder来实现字符串的分包解码器，事实上，不同的解码器都是要跟着具体的协议走的，对于字符串来说，一般会使用Header-Content协议，即在ByteBuf字节流中，先存放的4个字节用来表示紧跟着它的字符串长度，随后的字节是具体的字符串，以此不断向后叠加。

```java
public class MyStringDecoder extends ReplayingDecoder<MyStringDecoder.Status> {

    enum Status {
        READ_LENGTH,
        GET_STR
    }

    private int length;

    public MyStringDecoder() {
        super(Status.READ_LENGTH);
    }

    protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) throws Exception {
        switch (state()) {
            case READ_LENGTH:
                length = in.readInt();
                checkpoint(Status.GET_STR);
                break;
            case GET_STR:
                byte[] bytes = new byte[length];
                in.readBytes(bytes);
                out.add(new String(bytes, "UTF-8"));
                checkpoint(Status.READ_LENGTH);
                break;
            default:
                break;
        }
    }
}
```

但在实际应用中，不建议使用ReplayingDecoder，一个原因是并不是所有的ByteBuf都支持ReplayingDecoder，另一个原因在于checkpoint，当时数据不够时，会抛出异常，并会将读指针还原到原来的位置，在网络情况比较糟糕的情况下，可能出现一个数据包被解析多次，但是都没有解析成功的情况，对于CPU来说是比较大的消耗。

因此，更加建议的是直接继承ByteToMessageDecoder，并且对于不同的情况自己来实现数据不够时的处理逻辑，减少异常和指针的频繁移动。

#### MessageToMessageDecoder

前面介绍的Decoder针对的是ByteBuf数据转换为POJO对象的过程，那么假如需要从POJO对象转换为POJO对象，可以使用MessageToMessageDecoder，不同的是，它需要接收一个泛型I，来判断需要处理的POJO对象是哪种，实际用法和前面的也比较类似。

```java
public abstract class MessageToMessageDecoder<I> extends ChannelInboundHandlerAdapter {}
```

#### Netty内置的Decoder

##### FixedLengthFrameDecoder--固定长度数据包解码器

这个解码器会将数据拆分成一个个固定长度的数据包，然后发往下一个入站处理器

##### LineBasedFrameDecoder--行分割数据包解码器

这个解码器会将数据按照换行符“\n”或"\r\n"拆分成数据包，然后发往下一个入站处理器

##### DelimiterBasedFrameDecoder--自定义分隔符数据包解码器

##### LengthFieldBasedFrameDecoder--自定义长度数据包解码器

这个是用的比较多的一个解码器，如果是普通的Header-Content协议，也比较建议使用这个来实现，支持很多参数用来对协议进行不同程度的定制

```java
public class LengthFieldBasedFrameDecoder extends ByteToMessageDecoder {
    public LengthFieldBasedFrameDecoder(
            int maxFrameLength, int lengthFieldOffset, int lengthFieldLength,
            int lengthAdjustment, int initialBytesToStrip, boolean failFast) {
        this(
                ByteOrder.BIG_ENDIAN, maxFrameLength, lengthFieldOffset, lengthFieldLength,
                lengthAdjustment, initialBytesToStrip, failFast);
    }
}
```

- maxFrameLength，一个数据包的最大长度
- lengthFieldOffset，标识数据长度字段的偏移量（在整个数据包字节数组的下标值）
- lengthFieldLength，标识数据长度的字段所占的字节数，一般会有4表示int或2表示short
- lengthAdjustment，长度校正值=内容字段偏移量-长度字段偏移量-长度字段字节数，换句话说可以理解为长度校正值就是在长度字段和实际Content中间部分数据的长度，在传输协议比较复杂的情况下，可以在这段数据中写入协议版本号、时间戳等等信息
- initialBytesToStrip，在有效数据字段Content前的一些字节（可能包含长度字段、矫正字段等），可以通过这个值来设置是否保留这些数据
- failFast，快速失败，默认为true，在读取的数据长度刚刚要超过maxFrameLength时，立刻抛出异常；假如为false，则会等整个数据包完全读取完毕后再抛出异常



#### MessageToByteEncoder

编码器和解码器事实上很像，不同点在于编码器是出站处理器，需要子类实现encode方法，将泛型类型的POJO对象转换成字节数据并写入ByteBuf中

```java
protected abstract void encode(ChannelHandlerContext ctx, I msg, ByteBuf out) throws Exception;
```

#### MessageToMessageEncoder

同样的，编码器也支持POJO对象转换成POJO对象，以支持开发者进行自定义的处理



#### ByteToMessageCodec编解码器

在上面的例子中，我们将解码器和编码器分开成两个类，相应的，它们也需要分别被加入到Pipeline中，那么是不是可以把解码和编码放在一个类中，一次放入Pipeline呢，答案就是使用ByteToMessageCodec，并且需要在子类中同时实现encode和decode方法。

```java
public abstract class ByteToMessageCodec<I> extends ChannelDuplexHandler {
    protected abstract void encode(ChannelHandlerContext ctx, I msg, ByteBuf out) throws Exception;
    protected abstract void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) throws Exception;
}
```

```java
//ChannelDuplexHandler继承了入站基类，并实现了出站接口，使得它放到pipeline中时既可以做入站处理，也可以做出站处理
public class ChannelDuplexHandler extends ChannelInboundHandlerAdapter implements ChannelOutboundHandler
```



#### CombinedChannelDuplexHandler

Codec编解码器将编码和解码的操作强制放在同一个类中，对于只需要编码或解码单边操作的流水线上，使用codec不太合适，使用CombinedChannelDuplexHandler可以把编码器和解码器组合起来使用，但是不需要吧编码和解码逻辑放在同一个类中，既可以组合使用，也可以单独使用

```java
public class CombinedChannelDuplexHandler<I extends ChannelInboundHandler, O extends ChannelOutboundHandler>
        extends ChannelDuplexHandler {
    //构造函数同时接收入站和出站处理器将两个处理器组合起来
		public CombinedChannelDuplexHandler(I inboundHandler, O outboundHandler) {
    }
}
```





