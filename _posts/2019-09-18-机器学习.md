---

layout:     post
title:      机器学习入门
subtitle:   机器学习
date:       2019-09-18
author:     skaleto
catalog: true
tags:
    - 机器学习

---

# 机器学习入门

[TOC]



## 什么是机器学习

机器学习关注的问题：计算机程序如何随着经验积累自动提高性能

对于某类任务T和性能度量P，如果计算机程序在T上以P衡量的性能随着经验E而自我完善，那么计算机程序在从经验E学习。

模式识别起源于工程学，而机器学习产生于计算机科学，但是这些领域可以看成是同一领域的两个方面

## 机器学习类型

#### 监督学习

从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。

训练集主要包括输入和输出，也可以说是特征和目标，目标是由人工标注的。

主要用于分类和回归等问题



#### 无监督学习

输入数据没有人为标注的结果，模型对数据结构和数值进行归纳



#### 强化学习

输入数据可以刺激模型并使模型作出反应。

反馈不仅从监督学习的过程中得到，也会从环境的奖励惩罚中得到。



## 监督学习

##### 两类监督学习

分类：预测结果是离散的，例如判断是否是某种状态等等

回归：预测结果是连续的，例如预测年龄收入等

##### 监督学习的基本流程

![1568787106403](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568787106403.png)

##### 典型分类算法

决策树，朴素贝叶斯，随机森林，逻辑回归

##### 分类模型的评价

准确率，精确率，召回率，F1 score，ROC曲线，AUC曲线，混淆矩阵



##### 典型回归方法

线性回归，支持向量回归，分类回归树

##### 回归问题的评价

平均绝对误差，均方根误差，确定性系数



### 无监督学习：聚类



##### 典型聚类算法：Kmeans（K均值）

![1568787799449](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568787799449.png)



### 行业通用的机器学习算法选择

![1568787721708](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568787721708.png)





## 数据处理

机器学习建模需要高质量的数据：准确、完整、一致、时效、可信

#### 样本级数据处理

样本可以理解为普通的二维样本，类似表格

##### 样本选择

存在大量的无意义样本时（比如设备异常检测场景下的设备实时数据），需要对样本进行挑选，使不同类型的样本达到均衡

简单去重->样本可视化->规则性去重->随机过滤

- 简单去重，几乎完全相同的样本用阈值来控制去掉
- 样本可视化，将样本按特征显示出来，去掉变化规律几乎一致的样本
- 规则性去重，设定一系列基于人工的先验规则，过滤样本，规则基于深厚的业务场景理解

##### 样本生成

样本缺失严重、样本对于连续性敏感时需要生成一部分的样本，目的是样本完整性

- 统计值填充，某样本统计值缺失，在样本的附近或者全局按照统计量（均值、中位数、最大最小值）等进行缺失值的补充
- K最近邻填充，用K个最近（时间或空间上）的样本均值进行样本生成
- GAN生成，采用GAN网络进行样本生成（对抗生成网络）



#### 特征级数据处理

特征是表征一个样本最重要的部分

- 特征清洗，特征上缺失值，异常值的处理
  - 忽略
  - 缺失值填充
  - 异常值处理
  - 分箱平滑
  - 回归平滑

![1568797166614](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568797166614.png)

- 特征生成，特征提取、组合、映射
  - 特征组合，用原始特征进行相加、相乘生成新的特征
  - 特征统计，用一些特征的统计量或特殊的计算规则生成新的特征
  - 特征移位，主要是在一些时间相关的场景中

![1568797284707](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568797284707.png)

- 特征选择，特征间相关性的分析，进行特征的取舍
  - 主成分分析，在N维特征经过PCA搜索K个最能代表数据信息的维度，在搜索计算的过程中维度本身发生了改变，产生的是原始维度的映射，将映射后的特征进行重要性排序
    - 在每个特征上减去特征平均值
    - 计算数据集的协方差矩阵
    - 计算协方差矩阵的特征值和特征向量
    - 将特征值排序保留最大的N歌特征值
    - 将数据转换到特征向量构建的新空间中
  - 特征子集，当存在n个原始特征时，有2^n个特征子集，在n个特征进行筛选时，可以使用统计显著性检验的方法、信息增益的方法、决策树的方法
    - 逐步向前，从空集开始，每次添加当前特征集中最有价值的特征
    - 逐步向后，从特征全集开始，每次删除最没有价值的特征
    - 决策树，构建一颗规定深度的决策树，将没有出现在树中的特征删除
  - 相关性分析：检验两个特征之间的相关性，比如正相关负相关等等。比如皮尔森系数，斯皮尔曼系数，卡方检验等



- 特征归约，将特征的表征（连续型、离散型）进行一致性处理，将特征划分到类似的空间中，消除彼此度量差异造成的影响
  - 离散化，将特征映射到某几个类别中
    - 等宽分箱
    - 等频分箱
    - 聚类分箱
    - 邻近分箱
  - 标准化，将特征归约到某一个区间
    - 最大最小值
    - Z-score
    - 和值比例
  - 光滑，用回归等技术去除特征上的噪声
  - 聚集，不关注每一条样本的特征，关注样本集分片之后的特征，比如日销售额归约为周销售额



#### 集合级的数据处理

很多数据集是一个二维表，行是样本，列是特征，但是有些数据是一个样本就是一个二维表，需要将本来由一个二维表表示一个样本转化为一行数据表示一个样本

##### 二维表->样本

- 二维表分片
- 每个分片在原始每个特征上进行计算，每个特征对应多个结果
- 每个分片变成一行数据
- 二维表变成多行数据

##### 训练数据划分

- 随机划分训练集、验证集、测试集
- 指定测试集，其余部分随机划分





## 机器学习中的分类问题

##### 定义

使用带有类别标签的训练数据通过一定的算法或规则得出一个针对类别标签的分类器，这个分类器对于不带标签的数据给出标签的预测结果。

##### 数据定义

训练数据：1个样本是1个n维向量X*=(x1,x2,x3,…,xn)，每1个样本拥有1个自己的标签Y*。则样本{<X1*，Y1*>, <X2*，Y2*>,….,<Xm*，Ym*>}构成了一个样本空间（mⅹ（n+1））。
预测数据：1个样本是1个n维向量X=(x1,x2,x3,…,xn)，没有标签Y。则样本{<X1>, <X2>,…., <Xk>}构成了一个样本空间（kⅹn）。

##### 过程

第一阶段进行分类规则、分类器的的学习，称为训练阶段。即在训练数据集上使用算法或规则得到一个样本X*与标签Y*的映射：Y*=f(X*)
第二阶段进行分类规则、分类器的应用与更新，称为预测阶段。即使用训练阶段的结果映射ƒ对于预测样本X进行处理：Y= ƒ(X)

##### 评估

在训练阶段当中，将带有类别标签的训练数据划分为训练集和验证集，每一次使用训练集得到一个ƒ ，即用验证集去进行验证。



### 决策树算法

决策树是一种类似于流程图的树形结构，其中每个非叶结点都表示在某一个特征或者特征组合上的测试，每一个分支表示该测试的一个条件输出，而每个叶结点存放一个标签。

![1568800038547](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568800038547.png)

![1568800053479](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568800053479.png)



#### 决策树分裂准则

##### 信息增益

基于信息论，定义数据D中的期望信息为Info(D)，某个特征A的期望信息为InfoA(D)，它们分别都有个各自的计算公式。
特征A上的信息增益定义为：Gain(A)=Info(D)-InfoA(D)。
每一次进行数据分裂时，计算当前数据上每个特征的信息增益，选择信息增益最大的特征作为分裂的测试。

##### 基尼指数

定义数据D中的不纯度信息为Gini(𝐷) ，基尼指数考虑对一个特征A进行二叉分裂，假设给定A一个二叉分裂sp1，则这种分裂的不纯度信息为GiniA_sp1(D)。
则这种分裂导致的不纯度的降低为△ Gini(A_sp1) = Gini(𝐷)−GiniA_sp1(D)
每一次进行数据分裂时，计算当前数据上每个特征的每一种二叉分裂的△ Gini，选择△ Gini最大的特征的分裂作为分裂的测试。



### 随机森林算法

决策树产出的是一棵树去进行分类判定，随机森林是使用抽样有放回的方式产生多个训练集，每个训练集产出一颗决策树，形成决策森林

##### 分类判定

可由每棵树投票产生

- 平等投票，每棵树权重相同，哪个类别得票多，就作为模型的判定类别
- 权重投票

##### 随机过程

随机森林第一步是随机产生多个数据集，假设森林中树的个数是k，原始训练集为D，一共m个样本，每个样本n个特征

- 随机样本

  对于D中的样本进行有放回抽样，抽样k次，产生k个小训练集，每个小训练集样本数小于m，每个样本特征数依然是n

- 随机特征

  对于D中的特征进行有放回抽样，抽样k此，产生k个小训练集，每个小训练集样本数等于m，每个样本特征数小于n

- 随机特征组合

  随机产生一个数字L，L<n，随机选择L个特征，随机从[-1,1]中产出系数，L个特征和系数产生F个线性组合

一般3种方式组合使用，最终产生k个小训练集中的每个样本数小于m，特征数小于n



随机森林第二步是随机进行分裂规则选择，每个小训练集进行决策树产出的时候，可以随机选择是使用信息增益还是基尼指数



随机森林第三步是随机进行分裂特征的指定，每个小训练集进行决策树产出的时候，进行到每一步的分裂计算时，随机产生一个小于n的数t，且t小于这个小训练集的特征数，然后在当前分裂上随机选择t个特征进行分裂规则的计算。



##### 随机森林的优点

1、准确率比单棵决策树高。
2、对于离群点更加鲁棒。（对一些误差较大的数据兼容性更好）
3、随着森林中树个数的增加，森林的泛化误差收敛。
4、每棵树由于样本数和特征数都比原始训练集小，计算简单。
5、随机森林使用随机过程保持了每棵树的独立性。
6、随机森林在大型数据上非常有效。
7、随机森林具有强可解释性，可以给出特征重要性的内在估计。（每一棵树经过哪些分裂得到哪些结果都能够看到，可以估计每种特征的重要性）





### 分类评估指标的计算

#### 分类模型基本评估指标

定量的对模型的效果进行分析，对模型进行准确的评估

假定一个二分类问题，标签是<0，1>两种。如果是一个多分类问题，则可以站在某一个类别标签的角度看待其它所有的类别标签都可以归为“其它类”的范畴，将多分类转换为二分类。

假定站在标签0的角度。
P：标签为0的样本个数
N：标签为1的样本个数
TP：标签为0且模型判定为0的样本个数
TN：标签为1且模型判定为1的样本个数
FP：标签为1且模型判定为0的样本个数
FN：标签为0且模型判定为1的样本个数
𝛾：非负实数，为了赋予precision和recall不同的



##### 准确率

TP+TN  / P+N，所有正确分类和样本总数的比率

##### 错误率

FP+FN  /  P+N，所有错误分类和样本总数的比率

##### 召回率

TP / P，P这种样本是我们比较关注一种类别，那么对这种类别单独计算的比率就是召回率

##### 真负利率

TN / N

##### 精度

TP / TP+FP

##### F分数

2X精度X召回率 / 精度+召回率，即精度和召回率的平均

##### Fβ分数

为精度和召回率给予不同的权重并计算



评估指标的使用原则：

- 确立标签重要性，要确定哪一个类别是要非常关注的类别
- 重要类别的召回率、精度需要制定一个高的标准，比如达到xx%
- 非重要类别的指标尽可能的高



#### 混淆矩阵

当面对多分类问题，且每个类别的权重几乎等同时，利用混淆矩阵进行模型评估

![1568862345023](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568862345023.png)

从上面的图中可以看出，显然矩阵对角线上的值越大越好，其他位置上的值是越小越好



#### ROC曲线

当一些分类器（比如基于神经网络）给出的判定值并不是类别标签，而是一些数值的时候，就需要给这些数值一个阈值来产生类别，阈值不同，则类别判定结果也不同，这样就产生了基于阈值的recall（召回率）和1-specificity（真负利率），每一对这样的两个值可看做平面上的一个点，最终组合成ROC曲线。

那么理想情况下肯定是recall越高越好，真负利率越低越好，所以从图像上来看是曲线越凸越好

![1568863106706](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568863106706.png)



#### AUC值

AUC是ROC曲线所覆盖的区域面积，AUC越大则分类器效果越好

AUC=1：是完美分类器

0.5<AUC<1：优于随机猜测，如果妥善设定阈值，能有预测价值

AUC=0.5：跟随机猜测一样

AUC<0.5：差于随机猜测（但是反向预测就可以了）





### 逻辑回归算法 Logistic Regression

在二分类问题中，假定类别与类别之间差异巨大，将类别标签定义为<0,1>，赋予0和1数值意义，我们希望存在一个函数f，可以根据特征计算出位于区间[0,1]的值，根据阈值赋予类别标签。

![1568863874409](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568863874409.png)



机器学习中样本特征很多，特征数n比较大，LR计算的本质是在n+1维空间中使用Sigmoid function对训练样本集进行拟合，在拟合过程中对每个特征赋予不同的权重，如下图：

![1568864323858](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568864323858.png)

拟合过程即对所有的特征x1~xm，分别给定w0~wm的权重，对他们进行加权求和，最后再用Sigmoid function计算出最终值y

那么对于我们来说，我们一开始并不知道w0~wm需要设置成什么样的权重，但是我们知道最终需要的y值是0或者1，那么我们需要的就是一个拟合w0~wm的操作

拟合过程中我们需要一个评判标准，来判断这些w是不是我们正确的，有这样一个公式

![1568864982217](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568864982217.png)

这个公式即预测值-真实值的均方误差，称为一种代价函数

![1568865069608](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568865069608.png)

![1568865097503](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568865097503.png)





### 支持向量机SVM

考虑一个最简单的二分类问题，样本特征只有两个连续值型维度，这些样本可以画在二维平面上，假设训练样本比较均衡，并且不同类别之间区别较大，那么所有样本点呈现在平面上时，可以清晰看到明显分隔。

![1568873905957](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568873905957.png)

#### SVM的构想

能够进行样本分隔的直线只和位于类别边界上的样本相关，希望通过一些方法找到一条最佳的直线，当把数据推广到超过3维的空间，希望找到的就是一个平面，称为MMH（Maximum Marginal Hyperplane，最大边缘超平面）

![1568874082171](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568874082171.png)

#### SVM的计算

![1568874386673](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568874386673.png)

从图上可以看出，我们如果要找到这样一个平面MMH，或者简单点理解为一条直线，需要做的就是找到这样两个点，能够构造两条平行的直线，这两条平行直线之间的垂直距离最长。那么，能够构造这两条直线的两个点，或者说两个样本，就被称为支持向量，例如上图的构造H1和构造H2的两个点。

#### 线性不可分时SVM的计算

上图很明显是一种理想情况，我们的两种分类很配合地分布在两个位置，但如果两种分类杂糅在一起，在平面上展现不出明显的边界，该怎么办呢。这种情况称为线性不可分。我们采用下面的步骤：

1.使用非线性的映射将原始输入数据变换到更高维的空间

2.在新的高维空间进行MMH的求解

![1568875975393](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568875975393.png)



下面是一个非线性可分数据的例子

![](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=3267455404,2819185273&fm=173&app=25&f=JPEG?w=550&h=427&s=449A0E3A79104DCA0DCC2DCB0200C0B1)

我们将上图中的数据投影到一个三维空间：

![img](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=2287620649,3926631288&fm=173&app=25&f=JPEG?w=169&h=139&s=3AA8782339156DC85C5D00DF0000C0B1)

下面是投影到三维空间的数据。你是不是看到了一个可以悄悄放入一个平面的地方？

![img](https://hiphotos.baidu.com/feed/pic/item/bd3eb13533fa828bef66f322f11f4134960a5aad.jpg)

![](https://hiphotos.baidu.com/feed/pic/item/024f78f0f736afc3e82851a4bf19ebc4b64512ce.jpg)

上面我是为了展示投影到高维空间是如何工作的，所以选择了一个特定的投影。一般而言，很难找到这样的特定投影。不过，感谢Cover定理，我们确实知道，投影到高维空间后，数据更可能线性可分。实际上，我们可以将数据投影到 **无穷（infinite）**维，通常而言效果很好。

**所以我首先投影数据接着运行SVM？**

否。为了让上面的例子易于理解，我首先投影了数据。其实你只需让SVM为你投影数据。这带来了一些优势，包括SVM将使用一种称为 **核（kernels）**的东西进行投影，这相当迅速

#### 核

核基于向量的内积来做计算，接受原始空间中的两个数据点，给出投影空间中的点积。用核函数计算所需内积要快得多，如果数据点有许多维度，投影空间的维度更高，在大型数据集上，核函数节省的算力将飞速累积。这是核函数的巨大优势。

大多数SVM库内置了流行的核函数，比如 **多项式（Polynomial）**、 **径向基函数（Radial Basis Function，RBF）** 、 **Sigmoid** 。当我们不进行投影时，我们直接在原始空间计算点积——我们把这叫做使用 **线性核（linear kernel）** 。





## 机器学习中的回归问题

在分类问题中，我们会对一些数据集进行训练，得出若干个分类，最终需要解决的问题也是对于某种输入数据，它将属于哪种分类。而回归问题则是希望通过一些数据集得出一种回归的模型，使得我们输入某些数据后，能够得到一些其他的数据或值，比如通过历史销售额推测本月销售额等等。

一个预测问题在回归模型下的解决步骤为：数据收集，学习训练，回归预测

典型的回归方法：线性回归，KNN回归，分类回归树（基于平方误差）。。。

回归问题的评价：平均绝对误差，均方根误差，均方误差。。。



### 线性回归

与我们认知中的线性回归一致，不多赘述

![1568882692209](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568882692209.png)

![1569466622427](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569466622427.png)

上图h函数就是我们的回归预测模型，j函数是对于h函数的代价函数

线性回归算法中使用系数权重来加权每个特征变量的重要性，使用梯度下降来确定这些权重和偏置

#### 梯度下降

是一个一阶最优化算法，也称为最速下降法， 要使用梯度下降法找到一个函数的局部最小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。

可以类比成下山问题，每次选择该点附近最陡峭的地方下山，这样我们就可以最快速度地下到山底。但是对于存在两个局部最优解的情况下，如下图，从不同的点出发，最终可能到达两个不同的底部（局部最优解）。

![1569464991077](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569464991077.png)

梯度下降的算法本质上如下图，重复求解某一个点及这个点的导数与学习率的乘积之差，使这个点不断逼近极小值。

![1569466348981](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569466348981.png)

由于不断逼近极小值，某个点的导数值也会越来越小，那么，在学习率α不变的情况下，每次的步长也会越来越小。

![1569466427335](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569466427335.png)

![img](https://upload-images.jianshu.io/upload_images/13518408-f569aff3d53b6327.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)



#### 线性回归的特点

1.适用于预测目标与特征之间线性关系强的数据集。
2.有直观的理解和解释。
3.线性回归对异常值非常敏感。
4.避免函数梯度下降到局部最优解，需要进行标准化处理（将特征归约到某一个区间）。
5.梯度下降学习率a的选择不合适会出现“之字型”下降。
6.梯度下降在靠近极小值时速度减慢



### K最近邻回归（KNN）

最近邻我们可以理解为一个矩阵，矩阵中的某个点最近的四个点就是这个点的最近邻。

![1568884752417](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568884752417.png)

在上图中，我们假设实线圆的半径为4（K=4）：黄色圆点最接近的3个点是2个绿色三角形和1个红色正方形，少数服从多数，我们可以认为黄色圆点是属于绿色三角形类别。
我们假设虚线圆的半径为8（K=8）：黄色圆点最接近的5个点是2个绿色三角形和3个红色正方形，少数服从多数，我们可以认为黄色圆点是属于红色正方形类别。



于此我们看到，当无法判定当前待识别数据是从属于已知类别中的哪一类别时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归类(或分配)到权重更大的那一类。
这就是K近邻算法的核心思想。
KNN算法的核心更加像是分类问题，它不仅可以用于解决分类问题，也可以解决回归问题，KNN用在回归问题上时是采用分类思想来解决回归问题。将待测样本的值回归为从属类别的均值



#### KNN计算距离的方式

最常见的是欧式距离

![1568884944864](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568884944864.png)

KNN的主要优点有：
1.理论成熟，思想简单，既可以用来做分类也可以用来做回归。
2.可用于非线性分类。
3.训练时间复杂度比支持向量机之类的算法低，仅为O(n)。
4.和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感。
5.由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。
6.该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。

KNN的主要缺点有：
1.计算量大，尤其是特征数非常多的时候。
2.样本不平衡的时候，对稀有类别的预测准确率低。
3.使用懒散学习方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢。
4.相比决策树模型，KNN模型可解释性不强。



### 随机森林回归

我们上面在学习分类的时候，也有一个随机森林，里面的一颗颗决策树，每颗决策树的非叶节点如何分裂是根据信息增益和基尼指数，但在回归问题中就不太一样，虽然也有类似的树，叫做回归树，但每个非叶节点如何分裂是根据最小均方差来评判。

![1568885536689](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568885536689.png)

回归树的构建过程：

1.考虑数据集 R 上的所有特征 j，遍历每一个特征下所有可能的取值或者切分点 s，将数据集 R 划分成两部分 𝑅 1 和 𝑅 2
2.分别计算上述两个子集的平方误差和，选择最小的平方误差对应的特征与分割点，生成两个子节点。
3.对上述两个子节点递归调用步骤1和2,直到满足停止条件（1.分裂持续的次数；2.通过分箱来判断两个分裂的结果是否会被分到同一个箱，如果会被分到同一个箱，说明这个时候已经不需要继续分裂了）。



对于集合中的每个特征，都会产生很多的回归树，我们把每个回归树产生的结果组合起来就可以得到一个结果的集合。



### 梯度提升

梯度提升树回归是一种基于决策树的迭代回归算法。该算法采用迭代的思想不断地构建回归决策树模型，每棵树都是通过梯度优化损失函数而构建，从而达到从基准值到目标值的逼近。首先，我们以之前学习的线性回归为例来介绍梯度提升的概念，如下图所示，我们每一次的迭代建模都是：后一次模型都是针对前一次模型预测出错的情况进行修正，模型随着迭代不断地改进，从而获得比较好的预测效果。

![1568886264058](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568886264058.png)

![1568886638861](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568886638861.png)

![1568886653914](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568886653914.png)



但是我们发现一个问题，以上两种回归算法，最终得到的模型，在对某种输入去做预测时，输出的结果永远都在原始数据集的值域当中，不可能预测出不在原始数据集的值域中。这个是和线性回归不同的地方，线性回归我们是可以得到不在原始值域中的结果。

但是以上两种回归算法既然没有办法突破值域的限制，为什么还要使用它呢？那是因为，树形结构的回归树在回归的过程中有非常强烈的可解释性，比如上面解释为什么16岁没有驾驶证的打游戏多，可以从两颗回归树看出来，这种判断是有一定的依据的。而线性回归虽然突破了值域的限制，但当真的产生了突破值域的结果，我们其实没有一个很好的解释来说明为什么会产生这个结果。





## 机器学习中的聚类问题

聚类是按照某个特定标准把一个数据集分割成不同的类或簇，使同一个簇内的数据对象的相似性尽可能大，同时不再同一个簇中的数据对象差异性也尽可能的大。

聚类和分类的区别：

- 分类，对于一个分类器，通常需要我们体现给定某几种类别，使分类器可以从得到的训练集中进行学习，从而具备对未知数据进行分类的能力。这种训练方式是监督学习
- 聚类，是无监督学习，即我们不会事先给定一些类别，也并不关心某一类到底是什么，我们的目的只是要把相似的东西聚到一起。一般来说，一个聚类算法通常只需要知道如何计算相似度就可以工作了，通常不需要使用训练数据进行学习。一般需要我么对聚类的数据进一步分析来给他们赋予意义。



### 基于划分的聚类

##### 定义

给定一个n个对象的集合，将其按照一定的规则划分为k个分区，其中每个分区表示一个簇，k<=n，每个分区至少包含一个对象。基本划分方法是互斥的。
使用迭代的重定位技术：
1、设定规则。
2、给定一个初始划分。
3、根据规则计算这个划分包含的对象集。
4、更新划分。
5、循环3和4，直到无法更新划分

##### 方法

基于划分聚类有可能要穷举所有可能的划分。可以采用启发式的方法，逼近局部最优解。

##### 特点

容易发现高维空间中球形的互斥的簇

可以基于空间中样本的距离来判断数据的相似性

可以使用簇均值或者中心点代表簇



### 划分聚类算法：k-means原理

输入：k（簇的数目）、要进行分类的数据集D

输出：k个簇的集合

过程：

1. 从D中以某种规则选择k个样本或k个在值域范围内的点作为初始簇的中心
2. 计算簇中心之外的每个样本和每个簇中心的距离，将样本归属于最近的簇
3. 计算簇内均值，将均值作为簇的中心
4. 训练2和3（因为此时簇的中心发生了变化，原来属于这个簇的样本可能现在距离其他的簇中心更近了），知道簇中心的变化小于一定的阈值或即便簇中心变动，簇内样本也不变动

优缺点：

1. 能够快速收敛
2. 在大数据集上相对可伸缩和有效
3. 当无法计算均值时，可以通过其他的方式定义簇中心来改写算法（例如众数、中位数）
4. 严重依赖k值的确定，同时也依赖初始簇的确定
5. 不适合发现非球形簇或者大小差别非常大的簇
6. 对噪音数据十分敏感（因为默认取得是均值）



#### K值的设定

提供一些基于经验的k值，或k值范围，比较不同k得到的聚类结果来确定最佳k值

#### 初始簇心的选择

1. 随机初始簇心

2. 优先最大距离簇心

   1) 随机选择1个样本作为第1个簇心

   2) 随机选择m个样本，计算这些样本和当前所有簇心的距离dist

   3) 在2的计算结果中选择dist最大的样本作为下一个簇心

   4) 循环2和3，直到选出k个簇心

#### 终止规则

1. 完全终止，即在当前k值设定下，簇完全不发生更新时停止
2. 设定一个次数，当簇的更新次数达到这个值时停止





### 基于层次的聚类

在某些情况下，希望可以将数据划分成不同层次或者level上的簇。比如考虑某一种疾病的发病率，可以从个体发病率的角度去考虑、也可以从群体（比如性别）的角度去考虑。如果能够将数据在不同层次上进行聚类，则能发现更多的数据规律。
可以把所有数据看做一个大簇，也可以把每一个样本看做一个小簇。这样就衍生出了基于分裂的层次方法和基于凝聚的层次方法。
基于分裂的层次方法：自顶向下，迭代的进行分裂，从1个大簇形成多个小簇，最底层的簇都足够的凝聚（包含一个对象or 簇内的样本彼此都充分的相似）（一般情况下，分裂的依据是我们人为定的，并且没有科学的依据，所以基于分裂的层次方法一般来说准确性比较低）
基于凝聚的层次方法：自底向上，迭代的进行合并，从n个小簇形成更大的簇，终止条件可设定，也可以是最终形成一个簇。（一般可以通过计算样本的距离来判断样本相似性，比较相似的归为同一个簇）



#### 样本距离计算方法

![1568948998681](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1568948998681.png)





## 线性代数知识

### 矩阵向量的加减乘除



### 矩阵应用在数据集处理上

对于h(x)=a+bx的模型上，我们可以将一维的数据集，比如1,2,3,4这四个数据，转化成[1,1;1,2;1,3;1,4]这个4X2的矩阵向量，并且将h(x)的a和b这两个值转化成2X1的矩阵向量，将两个矩阵向量做乘积即可得到四组数据的模型计算结果矩阵，大小为4X1

当我们有三个模型时，我们可以将这三个模型的参数与我们的数据集组合起来求矩阵的乘积，得到最终的结果矩阵，如下图，这样的矩阵比较方便我们直观地看到每个数据在不同模型上的表现

![1569485290394](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569485290394.png)

#### 单位矩阵

即相同维度的矩阵与其单位矩阵的乘积仍然为其本身，例如2X2单位矩阵为[1,0;0,1]，3X3单位矩阵为[1,0,0;0,1,0;0,0,1]

#### 逆矩阵

在实数中，一个数的倒数与它的乘积始终为1，而在矩阵中，一个矩阵的逆矩阵与它的乘积始终为一个单位矩阵。

*也存在一些没有逆矩阵的矩阵，称为奇异矩阵或退化矩阵*

#### 矩阵的转置

例如一个mxn的矩阵A，将它的转换为nxm的矩阵B，此时A(ij)=B(ji)，B就称为A矩阵的转置



我们假设线性回归中的特征从一维增加到n维，那么我们的模型函数就会变成如下图

![1569489485571](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569489485571.png)

我们可以将上面的这些特征x0~xn表示为一个矩阵向量X，同时他们的系数表示为一个矩阵向量θ，那么模型函数等价于θ的转置与X的乘积。

#### 多特征梯度下降

对于多个特征的梯度下降算法，各个特征的更新值计算方式也和一维特征的类似

![1569491478851](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569491478851.png)



#### 特征缩放

举个栗子，假如我们(现在有特征x1，取值范围在0-2000，有特征x2，取值范围在1-5，那么它的J(θ)图像就会看到是一个非常细长的椭圆形，那么在固定步长下，达到最优解的过程可能是在图像上“左右横跳”，或者把步长尽可能缩小来减少左右横跳的次数，但是显然会严重增加训练时间。

![1569550290163](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569550290163.png)



特征缩放就是将所有特征值缩放到[-1,1]这个区间中，使每个特征的变化不会太大。当然也不一定要缩放到[-1,1]之间，事实上，只要是跨度不是特别大或特别小的区间，都是可以接受的，例如[-3,3]等等

#### 均值归一化

一种特征缩放的方式，将（xi-μi）/range作为xi归一化后的数值，μi表示该特征的平均值，range表示该特征的最大值-最小值。这个很容易理解，就是将某个特征平均缩放到某个区间上



在线性回归的梯度下降中，我们有这样一个公式

![1569552827152](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569552827152.png)

这个公式用于计算某个特征值下降之后的新值，其中有一个α代表学习率，上面说到，是一个经验值。

在比较理想的情况下，J(θ)的图像应该随着梯度下降次数的增加而下降，假如遇到图像呈现上升的趋势，或者图像存在上升和下降，那么就需要考虑降低学习率。在实际的训练中，可以尝试0.001,0.003,0.01,0.03,0.1这样的学习率

一般来说，可能会为J(θ)赋予一个阈值，当每次迭代降低的值小于这个值时，就认为这个梯度下降已经比较拟合了。

![1569553428608](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569553428608.png)

![1569553411309](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569553411309.png)





#### 多项式回归

线性回归中，我们除了使用一次函数，还可以使用类似二次函数，三次函数等多项式模型来做拟合，当然也可以使用平方根函数等等

![1569554702455](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569554702455.png)



#### 正规方程

假设我们的特征取值当前是一个实数，并且它的模型函数图像是一个二次函数，那么很显然，我们要求极小值时，只需要求出它的导数，并且令导数等于0，从而求出符合条件的值。

但如果我们的特征值是一个矩阵vector，那么我们要求的就是代价函数的极小值，我们当然可以也求出它的导函数，但是显然会比较复杂。

![1569555127590](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569555127590.png)

现在我们有这样一个公式可以得到一个最优的θ值，使得代价函数J(θ)可以最小化。

![1569555704960](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569555704960.png)

如果使用这个正规方程，那么数据不做归一化也可以，如果使用梯度下降，那么归一化是必要的。

从下图我们可以了解到梯度下降法和正规方程法的优劣：

##### 梯度下降

缺点：

​	学习率α需要我们凭经验指定

​	需要迭代很多次

优点：

​	当数据量特别大的时候，也能够正常运行

##### 正规方程

缺点：

​	数据量特别大的时候运行效率特别慢，因为要计算矩阵的逆，对于nxn的	矩阵，计算的时间复杂度是O(n3)，是相当耗时的

优点：

​	不需要凭经验来选择学习率

​	不需要多次迭代

![1569556157414](C:\Users\iflyrec\AppData\Roaming\Typora\typora-user-images\1569556157414.png)



但是同样存在几种情况下，X^T*X不存在逆矩阵

1. 存在冗余的特征，例如存在以平方米来描述的面积特征和以平方英尺来描述的面积特征，两者是可以线性转化的，从某种角度来看描述的是同一种特征
2. 特征数过多，比如特征数大于样本个数















